{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuttle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/5\n",
      "Run 2/5\n",
      "Run 3/5\n",
      "Run 4/5\n",
      "Run 5/5\n",
      "Average Test Accuracy: 0.8451\n",
      "Average Balanced Test Accuracy: 0.2866\n",
      "Average F2-Score (Weighted): 0.8163\n",
      "Average AUC-PR: 0.9602\n",
      "Average Matthews Correlation Coefficient: 0.5102\n",
      "\n",
      "Average Classification Report:\n",
      "Class 1:\n",
      "  precision: 0.8363\n",
      "  recall: 1.0000\n",
      "  f1-score: 0.9108\n",
      "  support: 9117.0000\n",
      "Class 2:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 10.0000\n",
      "Class 3:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 34.0000\n",
      "Class 4:\n",
      "  precision: 0.2000\n",
      "  recall: 0.0254\n",
      "  f1-score: 0.0450\n",
      "  support: 1781.0000\n",
      "Class 5:\n",
      "  precision: 0.9829\n",
      "  recall: 0.9810\n",
      "  f1-score: 0.9817\n",
      "  support: 653.0000\n",
      "Class 6:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 2.0000\n",
      "Class 7:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 3.0000\n",
      "Class macro avg:\n",
      "  precision: 0.2885\n",
      "  recall: 0.2866\n",
      "  f1-score: 0.2768\n",
      "  support: 11600.0000\n",
      "Class weighted avg:\n",
      "  precision: 0.7433\n",
      "  recall: 0.8451\n",
      "  f1-score: 0.7780\n",
      "  support: 11600.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from skeLCS import eLCS\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report\n",
    "from sklearn.metrics import fbeta_score, roc_auc_score, average_precision_score, matthews_corrcoef\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the training dataset\n",
    "train_data = pd.read_csv(\"shuttle_train.csv\")\n",
    "\n",
    "# Load the test dataset\n",
    "test_data = pd.read_csv(\"shuttle_test.csv\")\n",
    "\n",
    "# Parameters\n",
    "num_runs = 5  # Number of iterations\n",
    "\n",
    "# Initialize accumulators\n",
    "accuracy_scores = []\n",
    "balanced_accuracy_scores = []\n",
    "f2_scores = []\n",
    "#auc_roc_scores = []\n",
    "auc_pr_scores = []\n",
    "mcc_scores = []\n",
    "\n",
    "# Initialize metrics accumulators for classification report\n",
    "class_metrics = defaultdict(lambda: defaultdict(float))  # Stores precision/recall/F1 for each class\n",
    "\n",
    "for run in range(num_runs):\n",
    "    print(f\"Run {run + 1}/{num_runs}\")\n",
    "    \n",
    "    # Shuffle data\n",
    "    train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "    test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Replace infinities and fill NaN\n",
    "    train_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    train_data.fillna(train_data.mean(), inplace=True)\n",
    "    test_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    test_data.fillna(test_data.mean(), inplace=True)\n",
    "    \n",
    "    # Split features and labels\n",
    "    train_y = train_data['label'].values\n",
    "    train_X = train_data.drop(['label'], axis=1).values\n",
    "    test_y = test_data['label'].values\n",
    "    test_X = test_data.drop(['label'], axis=1).values\n",
    "    \n",
    "    # Train the model\n",
    "    model = eLCS()\n",
    "    trainedModel = model.fit(train_X,train_y)\n",
    "    \n",
    "    # Make predictions\n",
    "    test_predictions = model.predict(test_X)\n",
    "    test_probabilities = model.predict_proba(test_X)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(test_y, test_predictions)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    \n",
    "    # Calculate balanced accuracy\n",
    "    balanced_accuracy = balanced_accuracy_score(test_y, test_predictions)\n",
    "    balanced_accuracy_scores.append(balanced_accuracy)\n",
    "    \n",
    "    # Calculate F2-Score\n",
    "    f2_score = fbeta_score(test_y, test_predictions, beta=2, average='weighted')\n",
    "    f2_scores.append(f2_score)\n",
    "    \n",
    "    # Calculate AUC-ROC (One-vs-Rest)\n",
    "    #auc_roc = roc_auc_score(test_y, test_probabilities, multi_class='ovr')\n",
    "    #auc_roc_scores.append(auc_roc)\n",
    "    \n",
    "    # Calculate AUC-PR (One-vs-Rest)\n",
    "    auc_pr = average_precision_score(test_y, test_probabilities, average='weighted')\n",
    "    auc_pr_scores.append(auc_pr)\n",
    "    \n",
    "    # Calculate MCC\n",
    "    mcc = matthews_corrcoef(test_y, test_predictions)\n",
    "    mcc_scores.append(mcc)\n",
    "    \n",
    "    # Classification Report\n",
    "    report = classification_report(test_y, test_predictions, output_dict=True, zero_division=0)\n",
    "    \n",
    "    for class_label, metrics in report.items():\n",
    "        if isinstance(metrics, dict):  # Skip non-class metrics\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                class_metrics[class_label][metric_name] += metric_value\n",
    "\n",
    "# Calculate averages\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "balanced_average_accuracy = np.mean(balanced_accuracy_scores)\n",
    "average_f2_score = np.mean(f2_scores)\n",
    "#average_auc_roc = np.mean(auc_roc_scores)\n",
    "average_auc_pr = np.mean(auc_pr_scores)\n",
    "average_mcc = np.mean(mcc_scores)\n",
    "\n",
    "# Average the classification report metrics\n",
    "average_class_metrics = {\n",
    "    class_label: {metric_name: metric_value / num_runs for metric_name, metric_value in metrics.items()}\n",
    "    for class_label, metrics in class_metrics.items()\n",
    "}\n",
    "\n",
    "# Output results\n",
    "print(f\"Average Test Accuracy: {average_accuracy:.4f}\")\n",
    "print(f\"Average Balanced Test Accuracy: {balanced_average_accuracy:.4f}\")\n",
    "print(f\"Average F2-Score (Weighted): {average_f2_score:.4f}\")\n",
    "#print(f\"Average AUC-ROC (One-vs-Rest): {average_auc_roc:.4f}\")\n",
    "print(f\"Average AUC-PR: {average_auc_pr:.4f}\")\n",
    "print(f\"Average Matthews Correlation Coefficient: {average_mcc:.4f}\")\n",
    "\n",
    "print(\"\\nAverage Classification Report:\")\n",
    "for class_label, metrics in average_class_metrics.items():\n",
    "    print(f\"Class {class_label}:\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"  {metric_name}: {metric_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covertype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/5\n",
      "Run 2/5\n",
      "Run 3/5\n",
      "Run 4/5\n",
      "Run 5/5\n",
      "Average Test Accuracy: 0.6833\n",
      "Average Balanced Test Accuracy: 0.3532\n",
      "Average F2-Score (Weighted): 0.6722\n",
      "Average AUC-PR: 0.6916\n",
      "Average Matthews Correlation Coefficient: 0.4706\n",
      "\n",
      "Average Classification Report:\n",
      "Class 1:\n",
      "  precision: 0.6909\n",
      "  recall: 0.6171\n",
      "  f1-score: 0.6506\n",
      "  support: 42368.0000\n",
      "Class 2:\n",
      "  precision: 0.6904\n",
      "  recall: 0.8279\n",
      "  f1-score: 0.7525\n",
      "  support: 56661.0000\n",
      "Class 3:\n",
      "  precision: 0.6231\n",
      "  recall: 0.7174\n",
      "  f1-score: 0.6615\n",
      "  support: 7151.0000\n",
      "Class 4:\n",
      "  precision: 0.0571\n",
      "  recall: 0.0058\n",
      "  f1-score: 0.0106\n",
      "  support: 549.0000\n",
      "Class 5:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 1899.0000\n",
      "Class 6:\n",
      "  precision: 0.4011\n",
      "  recall: 0.0530\n",
      "  f1-score: 0.0784\n",
      "  support: 3473.0000\n",
      "Class 7:\n",
      "  precision: 0.7372\n",
      "  recall: 0.2513\n",
      "  f1-score: 0.3622\n",
      "  support: 4102.0000\n",
      "Class macro avg:\n",
      "  precision: 0.4571\n",
      "  recall: 0.3532\n",
      "  f1-score: 0.3594\n",
      "  support: 116203.0000\n",
      "Class weighted avg:\n",
      "  precision: 0.6652\n",
      "  recall: 0.6833\n",
      "  f1-score: 0.6600\n",
      "  support: 116203.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from skeLCS import eLCS\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import fbeta_score, roc_auc_score, average_precision_score, matthews_corrcoef\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the training dataset\n",
    "train_data = pd.read_csv(\"covtype_train.csv\")\n",
    "\n",
    "# Load the test dataset\n",
    "test_data = pd.read_csv(\"covtype_test.csv\")\n",
    "\n",
    "# Parameters\n",
    "num_runs = 5  # Number of iterations\n",
    "\n",
    "# Initialize accumulators\n",
    "accuracy_scores = []\n",
    "balanced_accuracy_scores = []\n",
    "f2_scores = []\n",
    "#auc_roc_scores = []\n",
    "auc_pr_scores = []\n",
    "mcc_scores = []\n",
    "\n",
    "# Initialize metrics accumulators for classification report\n",
    "class_metrics = defaultdict(lambda: defaultdict(float))  # Stores precision/recall/F1 for each class\n",
    "\n",
    "for run in range(num_runs):\n",
    "    print(f\"Run {run + 1}/{num_runs}\")\n",
    "    \n",
    "    # Shuffle data\n",
    "    train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "    test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Replace infinities and fill NaN\n",
    "    train_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    train_data.fillna(train_data.mean(), inplace=True)\n",
    "    test_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    test_data.fillna(test_data.mean(), inplace=True)\n",
    "    \n",
    "    # Split features and labels\n",
    "    train_y = train_data['label'].values\n",
    "    train_X = train_data.drop(['label'], axis=1).values\n",
    "    test_y = test_data['label'].values\n",
    "    test_X = test_data.drop(['label'], axis=1).values\n",
    "    \n",
    "    # Train the model\n",
    "    model = eLCS()\n",
    "    trainedModel = model.fit(train_X,train_y)\n",
    "    \n",
    "    # Make predictions\n",
    "    test_predictions = model.predict(test_X)\n",
    "    test_probabilities = model.predict_proba(test_X)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(test_y, test_predictions)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    \n",
    "    # Calculate balanced accuracy\n",
    "    balanced_accuracy = balanced_accuracy_score(test_y, test_predictions)\n",
    "    balanced_accuracy_scores.append(balanced_accuracy)\n",
    "    \n",
    "    # Calculate F2-Score\n",
    "    f2_score = fbeta_score(test_y, test_predictions, beta=2, average='weighted')\n",
    "    f2_scores.append(f2_score)\n",
    "    \n",
    "    # Calculate AUC-ROC (One-vs-Rest)\n",
    "    #auc_roc = roc_auc_score(test_y, test_probabilities, multi_class='ovr')\n",
    "    #auc_roc_scores.append(auc_roc)\n",
    "    \n",
    "    # Calculate AUC-PR (One-vs-Rest)\n",
    "    auc_pr = average_precision_score(test_y, test_probabilities, average='weighted')\n",
    "    auc_pr_scores.append(auc_pr)\n",
    "    \n",
    "    # Calculate MCC\n",
    "    mcc = matthews_corrcoef(test_y, test_predictions)\n",
    "    mcc_scores.append(mcc)\n",
    "    \n",
    "    # Classification Report\n",
    "    report = classification_report(test_y, test_predictions, output_dict=True, zero_division=0)\n",
    "    \n",
    "    for class_label, metrics in report.items():\n",
    "        if isinstance(metrics, dict):  # Skip non-class metrics\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                class_metrics[class_label][metric_name] += metric_value\n",
    "\n",
    "# Calculate averages\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "balanced_average_accuracy = np.mean(balanced_accuracy_scores)\n",
    "average_f2_score = np.mean(f2_scores)\n",
    "#average_auc_roc = np.mean(auc_roc_scores)\n",
    "average_auc_pr = np.mean(auc_pr_scores)\n",
    "average_mcc = np.mean(mcc_scores)\n",
    "\n",
    "# Average the classification report metrics\n",
    "average_class_metrics = {\n",
    "    class_label: {metric_name: metric_value / num_runs for metric_name, metric_value in metrics.items()}\n",
    "    for class_label, metrics in class_metrics.items()\n",
    "}\n",
    "\n",
    "# Output results\n",
    "print(f\"Average Test Accuracy: {average_accuracy:.4f}\")\n",
    "print(f\"Average Balanced Test Accuracy: {balanced_average_accuracy:.4f}\")\n",
    "print(f\"Average F2-Score (Weighted): {average_f2_score:.4f}\")\n",
    "#print(f\"Average AUC-ROC (One-vs-Rest): {average_auc_roc:.4f}\")\n",
    "print(f\"Average AUC-PR: {average_auc_pr:.4f}\")\n",
    "print(f\"Average Matthews Correlation Coefficient: {average_mcc:.4f}\")\n",
    "\n",
    "print(\"\\nAverage Classification Report:\")\n",
    "for class_label, metrics in average_class_metrics.items():\n",
    "    print(f\"Class {class_label}:\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"  {metric_name}: {metric_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/5\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shake\\AppData\\Local\\Temp\\ipykernel_20108\\3525449311.py:68: RuntimeWarning: invalid value encountered in divide\n",
      "  test_probabilities = test_probabilities / np.sum(test_probabilities, axis=1, keepdims=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 2/5\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shake\\AppData\\Local\\Temp\\ipykernel_20108\\3525449311.py:68: RuntimeWarning: invalid value encountered in divide\n",
      "  test_probabilities = test_probabilities / np.sum(test_probabilities, axis=1, keepdims=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 3/5\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shake\\AppData\\Local\\Temp\\ipykernel_20108\\3525449311.py:68: RuntimeWarning: invalid value encountered in divide\n",
      "  test_probabilities = test_probabilities / np.sum(test_probabilities, axis=1, keepdims=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4/5\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shake\\AppData\\Local\\Temp\\ipykernel_20108\\3525449311.py:68: RuntimeWarning: invalid value encountered in divide\n",
      "  test_probabilities = test_probabilities / np.sum(test_probabilities, axis=1, keepdims=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 5/5\n",
      "[[0. 1. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "[[0. 1. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shake\\AppData\\Local\\Temp\\ipykernel_20108\\3525449311.py:68: RuntimeWarning: invalid value encountered in divide\n",
      "  test_probabilities = test_probabilities / np.sum(test_probabilities, axis=1, keepdims=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Accuracy: 0.9898\n",
      "Average Balanced Test Accuracy: 0.2182\n",
      "Average F2-Score (Weighted): 0.9888\n",
      "Average AUC-PR: nan\n",
      "Average Matthews Correlation Coefficient: 0.8760\n",
      "\n",
      "Average Classification Report:\n",
      "Class 0:\n",
      "  precision: 0.9916\n",
      "  recall: 0.9996\n",
      "  f1-score: 0.9956\n",
      "  support: 194557.0000\n",
      "Class 1:\n",
      "  precision: 0.9856\n",
      "  recall: 0.8712\n",
      "  f1-score: 0.9248\n",
      "  support: 3178.0000\n",
      "Class 2:\n",
      "  precision: 0.8963\n",
      "  recall: 0.8801\n",
      "  f1-score: 0.8866\n",
      "  support: 2496.0000\n",
      "Class 3:\n",
      "  precision: 0.9601\n",
      "  recall: 0.9247\n",
      "  f1-score: 0.9418\n",
      "  support: 2083.0000\n",
      "Class 4:\n",
      "  precision: 0.9989\n",
      "  recall: 0.3806\n",
      "  f1-score: 0.5506\n",
      "  support: 463.0000\n",
      "Class 5:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 441.0000\n",
      "Class 6:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 204.0000\n",
      "Class 7:\n",
      "  precision: 0.7988\n",
      "  recall: 0.3102\n",
      "  f1-score: 0.3832\n",
      "  support: 196.0000\n",
      "Class 8:\n",
      "  precision: 0.4000\n",
      "  recall: 0.0528\n",
      "  f1-score: 0.0862\n",
      "  support: 53.0000\n",
      "Class 9:\n",
      "  precision: 0.2000\n",
      "  recall: 0.1636\n",
      "  f1-score: 0.1800\n",
      "  support: 11.0000\n",
      "Class 10:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 6.0000\n",
      "Class 11:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 4.0000\n",
      "Class 12:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 4.0000\n",
      "Class 13:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 2.0000\n",
      "Class 14:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 2.0000\n",
      "Class 15:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 2.0000\n",
      "Class 16:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 1.0000\n",
      "Class 17:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 2.0000\n",
      "Class 18:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 1.0000\n",
      "Class 19:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 1.0000\n",
      "Class 20:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 1.0000\n",
      "Class macro avg:\n",
      "  precision: 0.2967\n",
      "  recall: 0.2182\n",
      "  f1-score: 0.2357\n",
      "  support: 203708.0000\n",
      "Class weighted avg:\n",
      "  precision: 0.9864\n",
      "  recall: 0.9898\n",
      "  f1-score: 0.9875\n",
      "  support: 203708.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "d:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from skeLCS import eLCS\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report\n",
    "from sklearn.metrics import fbeta_score, roc_auc_score, average_precision_score, matthews_corrcoef\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define label mapping for KDD dataset labels\n",
    "label_mapping = {\n",
    "    'normal.': 0, 'satan.': 1, 'ipsweep.': 2, 'portsweep.': 3, 'nmap.': 4,\n",
    "    'back.': 5, 'warezclient.': 6, 'teardrop.': 7, 'pod.': 8, 'guess_passwd.': 9,\n",
    "    'buffer_overflow.': 10, 'land.': 11, 'warezmaster.': 12, 'imap.': 13, 'rootkit.': 14,\n",
    "    'loadmodule.': 15, 'multihop.': 16, 'ftp_write.': 17, 'phf.': 18, 'perl.': 19, 'spy.': 20\n",
    "}\n",
    "\n",
    "# Load the training dataset\n",
    "train_data = pd.read_csv(\"kdd_train.csv\")\n",
    "train_data['label'] = train_data['label'].map(label_mapping)  # Convert labels to numeric values\n",
    "\n",
    "# Load the test dataset\n",
    "test_data = pd.read_csv(\"kdd_test.csv\")\n",
    "test_data['label'] = test_data['label'].map(label_mapping)  # Convert labels to numeric values\n",
    "\n",
    "# Parameters\n",
    "num_runs = 5  # Number of iterations\n",
    "\n",
    "# Initialize accumulators\n",
    "accuracy_scores = []\n",
    "balanced_accuracy_scores = []\n",
    "f2_scores = []\n",
    "#auc_roc_scores = []\n",
    "auc_pr_scores = []\n",
    "mcc_scores = []\n",
    "\n",
    "# Initialize metrics accumulators for classification report\n",
    "class_metrics = defaultdict(lambda: defaultdict(float))  # Stores precision/recall/F1 for each class\n",
    "\n",
    "for run in range(num_runs):\n",
    "    print(f\"Run {run + 1}/{num_runs}\")\n",
    "    \n",
    "    # Shuffle data\n",
    "    train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "    test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Replace infinities and fill NaN\n",
    "    train_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    train_data.fillna(train_data.mean(), inplace=True)\n",
    "    test_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    test_data.fillna(test_data.mean(), inplace=True)\n",
    "    \n",
    "    # Split features and labels\n",
    "    train_y = train_data['label'].values\n",
    "    train_X = train_data.drop(['label'], axis=1).values\n",
    "    test_y = test_data['label'].values\n",
    "    test_X = test_data.drop(['label'], axis=1).values\n",
    "    \n",
    "    # Train the model\n",
    "    model = eLCS()\n",
    "    trainedModel = model.fit(train_X,train_y)\n",
    "    \n",
    "    # Make predictions\n",
    "    test_predictions = model.predict(test_X)\n",
    "    test_probabilities = model.predict_proba(test_X)\n",
    "\n",
    "    print(test_probabilities)  # Inspect the first few probability vectors\n",
    "    print(np.sum(test_probabilities, axis=1))  # Check if they sum to 1\n",
    "\n",
    "    test_probabilities = test_probabilities / np.sum(test_probabilities, axis=1, keepdims=True)\n",
    "    print(test_probabilities)  # Inspect the first few probability vectors\n",
    "    print(np.sum(test_probabilities, axis=1))  # Check if they sum to 1\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(test_y, test_predictions)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    \n",
    "    # Calculate balanced accuracy\n",
    "    balanced_accuracy = balanced_accuracy_score(test_y, test_predictions)\n",
    "    balanced_accuracy_scores.append(balanced_accuracy)\n",
    "    \n",
    "    # Calculate F2-Score\n",
    "    f2_score = fbeta_score(test_y, test_predictions, beta=2, average='weighted')\n",
    "    f2_scores.append(f2_score)\n",
    "    \n",
    "    # Calculate AUC-ROC (One-vs-Rest)\n",
    "    #auc_roc = roc_auc_score(test_y, test_probabilities, multi_class='ovr')\n",
    "    #auc_roc_scores.append(auc_roc)\n",
    "    \n",
    "    # Calculate AUC-PR (One-vs-Rest)\n",
    "    #auc_pr = average_precision_score(test_y, test_probabilities, average='weighted')\n",
    "    #auc_pr_scores.append(auc_pr)\n",
    "    \n",
    "    # Calculate MCC\n",
    "    mcc = matthews_corrcoef(test_y, test_predictions)\n",
    "    mcc_scores.append(mcc)\n",
    "    \n",
    "    # Classification Report\n",
    "    report = classification_report(test_y, test_predictions, output_dict=True, zero_division=0)\n",
    "    \n",
    "    for class_label, metrics in report.items():\n",
    "        if isinstance(metrics, dict):  # Skip non-class metrics\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                class_metrics[class_label][metric_name] += metric_value\n",
    "\n",
    "# Calculate averages\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "balanced_average_accuracy = np.mean(balanced_accuracy_scores)\n",
    "average_f2_score = np.mean(f2_scores)\n",
    "#average_auc_roc = np.mean(auc_roc_scores)\n",
    "average_auc_pr = np.mean(auc_pr_scores)\n",
    "average_mcc = np.mean(mcc_scores)\n",
    "\n",
    "# Average the classification report metrics\n",
    "average_class_metrics = {\n",
    "    class_label: {metric_name: metric_value / num_runs for metric_name, metric_value in metrics.items()}\n",
    "    for class_label, metrics in class_metrics.items()\n",
    "}\n",
    "\n",
    "# Output results\n",
    "print(f\"Average Test Accuracy: {average_accuracy:.4f}\")\n",
    "print(f\"Average Balanced Test Accuracy: {balanced_average_accuracy:.4f}\")\n",
    "print(f\"Average F2-Score (Weighted): {average_f2_score:.4f}\")\n",
    "#print(f\"Average AUC-ROC (One-vs-Rest): {average_auc_roc:.4f}\")\n",
    "print(f\"Average AUC-PR: {average_auc_pr:.4f}\")\n",
    "print(f\"Average Matthews Correlation Coefficient: {average_mcc:.4f}\")\n",
    "\n",
    "print(\"\\nAverage Classification Report:\")\n",
    "for class_label, metrics in average_class_metrics.items():\n",
    "    print(f\"Class {class_label}:\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"  {metric_name}: {metric_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Darknet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/5\n",
      "Run 2/5\n",
      "Run 3/5\n",
      "Run 4/5\n",
      "Run 5/5\n",
      "Average Test Accuracy: 0.9500\n",
      "Average Balanced Test Accuracy: 0.4195\n",
      "Average F2-Score (Weighted): 0.9451\n",
      "Average AUC-PR: 0.9570\n",
      "Average Matthews Correlation Coefficient: 0.8045\n",
      "\n",
      "Average Classification Report:\n",
      "Class 0:\n",
      "  precision: 0.9522\n",
      "  recall: 0.9967\n",
      "  f1-score: 0.9739\n",
      "  support: 26862.0000\n",
      "Class 1:\n",
      "  precision: 0.9838\n",
      "  recall: 0.8820\n",
      "  f1-score: 0.9298\n",
      "  support: 2657.0000\n",
      "Class 2:\n",
      "  precision: 0.9495\n",
      "  recall: 0.8465\n",
      "  f1-score: 0.8949\n",
      "  support: 908.0000\n",
      "Class 3:\n",
      "  precision: 0.8665\n",
      "  recall: 0.3111\n",
      "  f1-score: 0.4111\n",
      "  support: 522.0000\n",
      "Class 4:\n",
      "  precision: 0.4907\n",
      "  recall: 0.0894\n",
      "  f1-score: 0.1455\n",
      "  support: 293.0000\n",
      "Class 5:\n",
      "  precision: 0.7527\n",
      "  recall: 0.1019\n",
      "  f1-score: 0.1777\n",
      "  support: 269.0000\n",
      "Class 6:\n",
      "  precision: 0.6189\n",
      "  recall: 0.1500\n",
      "  f1-score: 0.2400\n",
      "  support: 116.0000\n",
      "Class 7:\n",
      "  precision: 0.3337\n",
      "  recall: 0.0566\n",
      "  f1-score: 0.0914\n",
      "  support: 53.0000\n",
      "Class 8:\n",
      "  precision: 0.1285\n",
      "  recall: 0.3409\n",
      "  f1-score: 0.1800\n",
      "  support: 44.0000\n",
      "Class macro avg:\n",
      "  precision: 0.6752\n",
      "  recall: 0.4195\n",
      "  f1-score: 0.4494\n",
      "  support: 31724.0000\n",
      "Class weighted avg:\n",
      "  precision: 0.9440\n",
      "  recall: 0.9500\n",
      "  f1-score: 0.9390\n",
      "  support: 31724.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from skeLCS import eLCS\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report\n",
    "from sklearn.metrics import fbeta_score, roc_auc_score, average_precision_score, matthews_corrcoef\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define label mapping for Darknet dataset labels\n",
    "label_mapping = {\n",
    "    'Normal': 0, 'Darknet_Audio-Streaming': 1, 'Darknet_Chat': 2, 'Darknet_File-Transfer': 3, 'Darknet_VOIP': 4,\n",
    "    'Darknet_Video-Streaming': 5, 'Darknet_Email': 6, 'Darknet_Browsing': 7, 'Darknet_P2P': 8\n",
    "}\n",
    "\n",
    "# Load the training dataset\n",
    "train_data = pd.read_csv(\"darknet_train.csv\")\n",
    "train_data['label'] = train_data['label'].map(label_mapping)  # Convert labels to numeric values\n",
    "\n",
    "# Load the test dataset\n",
    "test_data = pd.read_csv(\"darknet_test.csv\")\n",
    "test_data['label'] = test_data['label'].map(label_mapping)  # Convert labels to numeric values\n",
    "\n",
    "# Parameters\n",
    "num_runs = 5  # Number of iterations\n",
    "\n",
    "# Initialize accumulators\n",
    "accuracy_scores = []\n",
    "balanced_accuracy_scores = []\n",
    "f2_scores = []\n",
    "#auc_roc_scores = []\n",
    "auc_pr_scores = []\n",
    "mcc_scores = []\n",
    "\n",
    "# Initialize metrics accumulators for classification report\n",
    "class_metrics = defaultdict(lambda: defaultdict(float))  # Stores precision/recall/F1 for each class\n",
    "\n",
    "for run in range(num_runs):\n",
    "    print(f\"Run {run + 1}/{num_runs}\")\n",
    "    \n",
    "    # Shuffle data\n",
    "    train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "    test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Replace infinities and fill NaN\n",
    "    train_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    train_data.fillna(train_data.mean(), inplace=True)\n",
    "    test_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    test_data.fillna(test_data.mean(), inplace=True)\n",
    "    \n",
    "    # Split features and labels\n",
    "    train_y = train_data['label'].values\n",
    "    train_X = train_data.drop(['label'], axis=1).values\n",
    "    test_y = test_data['label'].values\n",
    "    test_X = test_data.drop(['label'], axis=1).values\n",
    "    \n",
    "    # Train the model\n",
    "    model = eLCS()\n",
    "    trainedModel = model.fit(train_X,train_y)\n",
    "    \n",
    "    # Make predictions\n",
    "    test_predictions = model.predict(test_X)\n",
    "    test_probabilities = model.predict_proba(test_X)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(test_y, test_predictions)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    \n",
    "    # Calculate balanced accuracy\n",
    "    balanced_accuracy = balanced_accuracy_score(test_y, test_predictions)\n",
    "    balanced_accuracy_scores.append(balanced_accuracy)\n",
    "    \n",
    "    # Calculate F2-Score\n",
    "    f2_score = fbeta_score(test_y, test_predictions, beta=2, average='weighted')\n",
    "    f2_scores.append(f2_score)\n",
    "    \n",
    "    # Calculate AUC-ROC (One-vs-Rest)\n",
    "    #auc_roc = roc_auc_score(test_y, test_probabilities, multi_class='ovr')\n",
    "    #auc_roc_scores.append(auc_roc)\n",
    "    \n",
    "    # Calculate AUC-PR (One-vs-Rest)\n",
    "    auc_pr = average_precision_score(test_y, test_probabilities, average='weighted')\n",
    "    auc_pr_scores.append(auc_pr)\n",
    "    \n",
    "    # Calculate MCC\n",
    "    mcc = matthews_corrcoef(test_y, test_predictions)\n",
    "    mcc_scores.append(mcc)\n",
    "    \n",
    "    # Classification Report\n",
    "    report = classification_report(test_y, test_predictions, output_dict=True, zero_division=0)\n",
    "    \n",
    "    for class_label, metrics in report.items():\n",
    "        if isinstance(metrics, dict):  # Skip non-class metrics\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                class_metrics[class_label][metric_name] += metric_value\n",
    "\n",
    "# Calculate averages\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "balanced_average_accuracy = np.mean(balanced_accuracy_scores)\n",
    "average_f2_score = np.mean(f2_scores)\n",
    "#average_auc_roc = np.mean(auc_roc_scores)\n",
    "average_auc_pr = np.mean(auc_pr_scores)\n",
    "average_mcc = np.mean(mcc_scores)\n",
    "\n",
    "# Average the classification report metrics\n",
    "average_class_metrics = {\n",
    "    class_label: {metric_name: metric_value / num_runs for metric_name, metric_value in metrics.items()}\n",
    "    for class_label, metrics in class_metrics.items()\n",
    "}\n",
    "\n",
    "# Output results\n",
    "print(f\"Average Test Accuracy: {average_accuracy:.4f}\")\n",
    "print(f\"Average Balanced Test Accuracy: {balanced_average_accuracy:.4f}\")\n",
    "print(f\"Average F2-Score (Weighted): {average_f2_score:.4f}\")\n",
    "#print(f\"Average AUC-ROC (One-vs-Rest): {average_auc_roc:.4f}\")\n",
    "print(f\"Average AUC-PR: {average_auc_pr:.4f}\")\n",
    "print(f\"Average Matthews Correlation Coefficient: {average_mcc:.4f}\")\n",
    "\n",
    "print(\"\\nAverage Classification Report:\")\n",
    "for class_label, metrics in average_class_metrics.items():\n",
    "    print(f\"Class {class_label}:\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"  {metric_name}: {metric_value:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
