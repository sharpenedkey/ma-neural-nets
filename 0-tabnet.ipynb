{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import fbeta_score, roc_auc_score, average_precision_score, matthews_corrcoef\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def import_data(file):\n",
    "    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n",
    "    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuttle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.5214  | val_0_logloss: 0.06544 |  0:00:04s\n",
      "epoch 1  | loss: 0.05223 | val_0_logloss: 0.02861 |  0:00:10s\n",
      "epoch 2  | loss: 0.03198 | val_0_logloss: 0.02694 |  0:00:15s\n",
      "epoch 3  | loss: 0.04257 | val_0_logloss: 0.08513 |  0:00:20s\n",
      "epoch 4  | loss: 0.03237 | val_0_logloss: 0.02389 |  0:00:26s\n",
      "epoch 5  | loss: 0.02714 | val_0_logloss: 0.02176 |  0:00:31s\n",
      "epoch 6  | loss: 0.02316 | val_0_logloss: 0.01996 |  0:00:36s\n",
      "epoch 7  | loss: 0.02064 | val_0_logloss: 0.01694 |  0:00:42s\n",
      "epoch 8  | loss: 0.01606 | val_0_logloss: 0.01603 |  0:00:46s\n",
      "epoch 9  | loss: 0.01342 | val_0_logloss: 0.01474 |  0:00:51s\n",
      "epoch 10 | loss: 0.01426 | val_0_logloss: 0.01286 |  0:00:56s\n",
      "epoch 11 | loss: 0.01864 | val_0_logloss: 0.01677 |  0:01:01s\n",
      "epoch 12 | loss: 0.01391 | val_0_logloss: 0.01291 |  0:01:06s\n",
      "epoch 13 | loss: 0.01278 | val_0_logloss: 0.01126 |  0:01:11s\n",
      "epoch 14 | loss: 0.0118  | val_0_logloss: 0.01642 |  0:01:16s\n",
      "epoch 15 | loss: 0.01131 | val_0_logloss: 0.01249 |  0:01:23s\n",
      "epoch 16 | loss: 0.01193 | val_0_logloss: 0.01758 |  0:01:31s\n",
      "epoch 17 | loss: 0.01302 | val_0_logloss: 0.01706 |  0:01:35s\n",
      "epoch 18 | loss: 0.01205 | val_0_logloss: 0.0208  |  0:01:40s\n",
      "epoch 19 | loss: 0.01188 | val_0_logloss: 0.01469 |  0:01:45s\n",
      "epoch 20 | loss: 0.01152 | val_0_logloss: 0.01942 |  0:01:50s\n",
      "epoch 21 | loss: 0.0116  | val_0_logloss: 0.01179 |  0:01:55s\n",
      "epoch 22 | loss: 0.01397 | val_0_logloss: 0.01572 |  0:02:00s\n",
      "epoch 23 | loss: 0.01194 | val_0_logloss: 0.01704 |  0:02:05s\n",
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_0_logloss = 0.01126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 2/5\n",
      "\n",
      "Early stopping occurred at epoch 43 with best_epoch = 33 and best_val_0_logloss = 0.01127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 3/5\n",
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 15 and best_val_0_logloss = 0.01281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4/5\n",
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 14 and best_val_0_logloss = 0.01494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 5/5\n",
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 21 and best_val_0_logloss = 0.01452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Accuracy: 0.9969\n",
      "Average F2-Score (Weighted): 0.9966\n",
      "Average AUC-ROC (One-vs-Rest): 0.9926\n",
      "Average AUC-PR: 0.9976\n",
      "Average Matthews Correlation Coefficient: 0.9913\n",
      "\n",
      "Average Classification Report:\n",
      "Class 1:\n",
      "  precision: 0.9983\n",
      "  recall: 0.9998\n",
      "  f1-score: 0.9990\n",
      "  support: 9117.0000\n",
      "Class 2:\n",
      "  precision: 0.5967\n",
      "  recall: 0.3800\n",
      "  f1-score: 0.4154\n",
      "  support: 10.0000\n",
      "Class 3:\n",
      "  precision: 0.8043\n",
      "  recall: 0.3471\n",
      "  f1-score: 0.4821\n",
      "  support: 34.0000\n",
      "Class 4:\n",
      "  precision: 0.9954\n",
      "  recall: 1.0000\n",
      "  f1-score: 0.9977\n",
      "  support: 1781.0000\n",
      "Class 5:\n",
      "  precision: 0.9891\n",
      "  recall: 0.9982\n",
      "  f1-score: 0.9936\n",
      "  support: 653.0000\n",
      "Class 6:\n",
      "  precision: 0.3000\n",
      "  recall: 0.2000\n",
      "  f1-score: 0.2333\n",
      "  support: 2.0000\n",
      "Class 7:\n",
      "  precision: 0.2000\n",
      "  recall: 0.0667\n",
      "  f1-score: 0.1000\n",
      "  support: 3.0000\n",
      "Class macro avg:\n",
      "  precision: 0.6977\n",
      "  recall: 0.5702\n",
      "  f1-score: 0.6030\n",
      "  support: 11600.0000\n",
      "Class weighted avg:\n",
      "  precision: 0.9961\n",
      "  recall: 0.9969\n",
      "  f1-score: 0.9961\n",
      "  support: 11600.0000\n"
     ]
    }
   ],
   "source": [
    "# Load the training dataset\n",
    "train_data = pd.read_csv(\"shuttle_train.csv\")\n",
    "\n",
    "# Load the test dataset\n",
    "test_data = pd.read_csv(\"shuttle_test.csv\")\n",
    "\n",
    "# Parameters\n",
    "num_runs = 5  # Number of iterations\n",
    "\n",
    "# Initialize accumulators\n",
    "accuracy_scores = []\n",
    "f2_scores = []\n",
    "auc_roc_scores = []\n",
    "auc_pr_scores = []\n",
    "mcc_scores = []\n",
    "\n",
    "# Initialize metrics accumulators for classification report\n",
    "class_metrics = defaultdict(lambda: defaultdict(float))  # Stores precision/recall/F1 for each class\n",
    "\n",
    "verbose_run = 1\n",
    "for run in range(num_runs):\n",
    "    print(f\"Run {run + 1}/{num_runs}\")\n",
    "    \n",
    "    # Shuffle data\n",
    "    train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "    test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Split features and labels\n",
    "    train_y = train_data['label'].values\n",
    "    train_X = train_data.drop(['label'], axis=1).values\n",
    "    test_y = test_data['label'].values\n",
    "    test_X = test_data.drop(['label'], axis=1).values\n",
    "    \n",
    "    # Train the model\n",
    "    clf = TabNetClassifier(verbose=verbose_run)\n",
    "    clf.fit(train_X, train_y, eval_set=[(train_X, train_y)], eval_metric=['logloss'])\n",
    "    verbose_run = 0\n",
    "    \n",
    "    # Make predictions\n",
    "    test_probabilities = clf.predict_proba(test_X)  # Outputs probabilities\n",
    "    test_predictions = clf.predict(test_X)  # Convert to discrete class predictions\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(test_y, test_predictions)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    \n",
    "    # Calculate F2-Score\n",
    "    f2_score = fbeta_score(test_y, test_predictions, beta=2, average='weighted')\n",
    "    f2_scores.append(f2_score)\n",
    "    \n",
    "    # Calculate AUC-ROC (One-vs-Rest)\n",
    "    auc_roc = roc_auc_score(test_y, test_probabilities, multi_class='ovr')\n",
    "    auc_roc_scores.append(auc_roc)\n",
    "    \n",
    "    # Calculate AUC-PR (One-vs-Rest)\n",
    "    auc_pr = average_precision_score(test_y, test_probabilities, average='weighted')\n",
    "    auc_pr_scores.append(auc_pr)\n",
    "    \n",
    "    # Calculate MCC\n",
    "    mcc = matthews_corrcoef(test_y, test_predictions)\n",
    "    mcc_scores.append(mcc)\n",
    "    \n",
    "    # Classification Report\n",
    "    report = classification_report(test_y, test_predictions, output_dict=True, zero_division=0)\n",
    "    \n",
    "    for class_label, metrics in report.items():\n",
    "        if isinstance(metrics, dict):  # Skip non-class metrics\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                class_metrics[class_label][metric_name] += metric_value\n",
    "\n",
    "# Calculate averages\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "average_f2_score = np.mean(f2_scores)\n",
    "average_auc_roc = np.mean(auc_roc_scores)\n",
    "average_auc_pr = np.mean(auc_pr_scores)\n",
    "average_mcc = np.mean(mcc_scores)\n",
    "\n",
    "# Average the classification report metrics\n",
    "average_class_metrics = {\n",
    "    class_label: {metric_name: metric_value / num_runs for metric_name, metric_value in metrics.items()}\n",
    "    for class_label, metrics in class_metrics.items()\n",
    "}\n",
    "\n",
    "# Output results\n",
    "print(f\"Average Test Accuracy: {average_accuracy:.4f}\")\n",
    "print(f\"Average F2-Score (Weighted): {average_f2_score:.4f}\")\n",
    "print(f\"Average AUC-ROC (One-vs-Rest): {average_auc_roc:.4f}\")\n",
    "print(f\"Average AUC-PR: {average_auc_pr:.4f}\")\n",
    "print(f\"Average Matthews Correlation Coefficient: {average_mcc:.4f}\")\n",
    "\n",
    "print(\"\\nAverage Classification Report:\")\n",
    "for class_label, metrics in average_class_metrics.items():\n",
    "    print(f\"Class {class_label}:\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"  {metric_name}: {metric_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covertype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     34\u001b[0m clf \u001b[38;5;241m=\u001b[39m TabNetClassifier(verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 35\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m     38\u001b[0m test_probabilities \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(test_X)  \u001b[38;5;66;03m# Outputs probabilities\u001b[39;00m\n",
      "File \u001b[1;32md:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:258\u001b[0m, in \u001b[0;36mTabModel.fit\u001b[1;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations, compute_importance)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_epochs):\n\u001b[0;32m    254\u001b[0m \n\u001b[0;32m    255\u001b[0m     \u001b[38;5;66;03m# Call method on_epoch_begin for all callbacks\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_container\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch_idx)\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# Apply predict epoch to all eval sets\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m eval_name, valid_dataloader \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(eval_names, valid_dataloaders):\n",
      "File \u001b[1;32md:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:489\u001b[0m, in \u001b[0;36mTabModel._train_epoch\u001b[1;34m(self, train_loader)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m    487\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_container\u001b[38;5;241m.\u001b[39mon_batch_begin(batch_idx)\n\u001b[1;32m--> 489\u001b[0m     batch_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_container\u001b[38;5;241m.\u001b[39mon_batch_end(batch_idx, batch_logs)\n\u001b[0;32m    493\u001b[0m epoch_logs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n",
      "File \u001b[1;32md:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:527\u001b[0m, in \u001b[0;36mTabModel._train_batch\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[0;32m    525\u001b[0m     param\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 527\u001b[0m output, M_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    529\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(output, y)\n\u001b[0;32m    530\u001b[0m \u001b[38;5;66;03m# Add the overall sparsity loss\u001b[39;00m\n",
      "File \u001b[1;32md:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\pytorch_tabnet\\tab_network.py:616\u001b[0m, in \u001b[0;36mTabNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    615\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedder(x)\n\u001b[1;32m--> 616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtabnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\pytorch_tabnet\\tab_network.py:492\u001b[0m, in \u001b[0;36mTabNetNoEmbeddings.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    491\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 492\u001b[0m     steps_output, M_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m     res \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mstack(steps_output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_multi_task:\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;66;03m# Result will be in list format\u001b[39;00m\n",
      "File \u001b[1;32md:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\pytorch_tabnet\\tab_network.py:181\u001b[0m, in \u001b[0;36mTabNetEncoder.forward\u001b[1;34m(self, x, prior)\u001b[0m\n\u001b[0;32m    179\u001b[0m M_feature_level \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(M, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_attention_matrix)\n\u001b[0;32m    180\u001b[0m masked_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmul(M_feature_level, x)\n\u001b[1;32m--> 181\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeat_transformers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m d \u001b[38;5;241m=\u001b[39m ReLU()(out[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_d])\n\u001b[0;32m    183\u001b[0m steps_output\u001b[38;5;241m.\u001b[39mappend(d)\n",
      "File \u001b[1;32md:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\pytorch_tabnet\\tab_network.py:738\u001b[0m, in \u001b[0;36mFeatTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    737\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared(x)\n\u001b[1;32m--> 738\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspecifics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32md:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\pytorch_tabnet\\tab_network.py:780\u001b[0m, in \u001b[0;36mGLU_Block.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    777\u001b[0m     layers_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_glu)\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m glu_id \u001b[38;5;129;01min\u001b[39;00m layers_left:\n\u001b[1;32m--> 780\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglu_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mglu_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    781\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m scale\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the training dataset\n",
    "train_data = pd.read_csv(\"covtype_train.csv\")\n",
    "\n",
    "# Load the test dataset\n",
    "test_data = pd.read_csv(\"covtype_test.csv\")\n",
    "\n",
    "# Parameters\n",
    "num_runs = 5  # Number of iterations\n",
    "\n",
    "# Initialize accumulators\n",
    "accuracy_scores = []\n",
    "f2_scores = []\n",
    "auc_roc_scores = []\n",
    "auc_pr_scores = []\n",
    "mcc_scores = []\n",
    "\n",
    "# Initialize metrics accumulators for classification report\n",
    "class_metrics = defaultdict(lambda: defaultdict(float))  # Stores precision/recall/F1 for each class\n",
    "\n",
    "verbose_run = 1\n",
    "for run in range(num_runs):\n",
    "    print(f\"Run {run + 1}/{num_runs}\")\n",
    "    \n",
    "    # Shuffle data\n",
    "    train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "    test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Split features and labels\n",
    "    train_y = train_data['label'].values\n",
    "    train_X = train_data.drop(['label'], axis=1).values\n",
    "    test_y = test_data['label'].values\n",
    "    test_X = test_data.drop(['label'], axis=1).values\n",
    "    \n",
    "    # Train the model\n",
    "    clf = TabNetClassifier(verbose=verbose_run)\n",
    "    clf.fit(train_X, train_y, eval_set=[(train_X, train_y)], eval_metric=['logloss'])\n",
    "    verbose_run = 0\n",
    "    \n",
    "    # Make predictions\n",
    "    test_probabilities = clf.predict_proba(test_X)  # Outputs probabilities\n",
    "    test_predictions = clf.predict(test_X)  # Convert to discrete class predictions\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(test_y, test_predictions)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    \n",
    "    # Calculate F2-Score\n",
    "    f2_score = fbeta_score(test_y, test_predictions, beta=2, average='weighted')\n",
    "    f2_scores.append(f2_score)\n",
    "    \n",
    "    # Calculate AUC-ROC (One-vs-Rest)\n",
    "    auc_roc = roc_auc_score(test_y, test_probabilities, multi_class='ovr')\n",
    "    auc_roc_scores.append(auc_roc)\n",
    "    \n",
    "    # Calculate AUC-PR (One-vs-Rest)\n",
    "    auc_pr = average_precision_score(test_y, test_probabilities, average='weighted')\n",
    "    auc_pr_scores.append(auc_pr)\n",
    "    \n",
    "    # Calculate MCC\n",
    "    mcc = matthews_corrcoef(test_y, test_predictions)\n",
    "    mcc_scores.append(mcc)\n",
    "    \n",
    "    # Classification Report\n",
    "    report = classification_report(test_y, test_predictions, output_dict=True, zero_division=0)\n",
    "    \n",
    "    for class_label, metrics in report.items():\n",
    "        if isinstance(metrics, dict):  # Skip non-class metrics\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                class_metrics[class_label][metric_name] += metric_value\n",
    "\n",
    "# Calculate averages\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "average_f2_score = np.mean(f2_scores)\n",
    "average_auc_roc = np.mean(auc_roc_scores)\n",
    "average_auc_pr = np.mean(auc_pr_scores)\n",
    "average_mcc = np.mean(mcc_scores)\n",
    "\n",
    "# Average the classification report metrics\n",
    "average_class_metrics = {\n",
    "    class_label: {metric_name: metric_value / num_runs for metric_name, metric_value in metrics.items()}\n",
    "    for class_label, metrics in class_metrics.items()\n",
    "}\n",
    "\n",
    "# Output results\n",
    "print(f\"Average Test Accuracy: {average_accuracy:.4f}\")\n",
    "print(f\"Average F2-Score (Weighted): {average_f2_score:.4f}\")\n",
    "print(f\"Average AUC-ROC (One-vs-Rest): {average_auc_roc:.4f}\")\n",
    "print(f\"Average AUC-PR: {average_auc_pr:.4f}\")\n",
    "print(f\"Average Matthews Correlation Coefficient: {average_mcc:.4f}\")\n",
    "\n",
    "print(\"\\nAverage Classification Report:\")\n",
    "for class_label, metrics in average_class_metrics.items():\n",
    "    print(f\"Class {class_label}:\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"  {metric_name}: {metric_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9966\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "           back.       0.74      0.03      0.06       441\n",
      "buffer_overflow.       0.00      0.00      0.00         6\n",
      "      ftp_write.       0.00      0.00      0.00         2\n",
      "   guess_passwd.       1.00      0.91      0.95        11\n",
      "           imap.       0.50      0.50      0.50         2\n",
      "        ipsweep.       0.99      0.99      0.99      2496\n",
      "           land.       0.00      0.00      0.00         4\n",
      "     loadmodule.       0.00      0.00      0.00         2\n",
      "       multihop.       0.00      0.00      0.00         1\n",
      "           nmap.       0.99      0.92      0.96       463\n",
      "         normal.       1.00      1.00      1.00    194557\n",
      "           perl.       0.00      0.00      0.00         1\n",
      "            phf.       0.00      0.00      0.00         1\n",
      "            pod.       1.00      0.66      0.80        53\n",
      "      portsweep.       1.00      0.98      0.99      2083\n",
      "        rootkit.       0.00      0.00      0.00         2\n",
      "          satan.       1.00      0.99      0.99      3178\n",
      "            spy.       0.00      0.00      0.00         1\n",
      "       teardrop.       0.86      1.00      0.93       196\n",
      "    warezclient.       0.77      0.93      0.84       204\n",
      "    warezmaster.       0.00      0.00      0.00         4\n",
      "\n",
      "        accuracy                           1.00    203708\n",
      "       macro avg       0.47      0.42      0.43    203708\n",
      "    weighted avg       1.00      1.00      1.00    203708\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Load the training dataset\n",
    "train_data = import_data(\"kdd_train.csv\")\n",
    "\n",
    "# Load the test dataset\n",
    "test_data = import_data(\"kdd_test.csv\")\n",
    "\n",
    "# Parameters\n",
    "num_runs = 5  # Number of iterations\n",
    "\n",
    "# Initialize accumulators\n",
    "accuracy_scores = []\n",
    "f2_scores = []\n",
    "auc_roc_scores = []\n",
    "auc_pr_scores = []\n",
    "mcc_scores = []\n",
    "\n",
    "# Initialize metrics accumulators for classification report\n",
    "class_metrics = defaultdict(lambda: defaultdict(float))  # Stores precision/recall/F1 for each class\n",
    "\n",
    "verbose_run = 1\n",
    "for run in range(num_runs):\n",
    "    print(f\"Run {run + 1}/{num_runs}\")\n",
    "    \n",
    "    # Shuffle data\n",
    "    train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "    test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Split features and labels\n",
    "    train_y = train_data['label'].values\n",
    "    train_X = train_data.drop(['label'], axis=1).values\n",
    "    test_y = test_data['label'].values\n",
    "    test_X = test_data.drop(['label'], axis=1).values\n",
    "    \n",
    "    # Train the model\n",
    "    clf = TabNetClassifier(verbose=verbose_run)\n",
    "    clf.fit(train_X, train_y, eval_set=[(train_X, train_y)], eval_metric=['logloss'])\n",
    "    verbose_run = 0\n",
    "    \n",
    "    # Make predictions\n",
    "    test_probabilities = clf.predict_proba(test_X)  # Outputs probabilities\n",
    "    test_predictions = clf.predict(test_X)  # Convert to discrete class predictions\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(test_y, test_predictions)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    \n",
    "    # Calculate F2-Score\n",
    "    f2_score = fbeta_score(test_y, test_predictions, beta=2, average='weighted')\n",
    "    f2_scores.append(f2_score)\n",
    "    \n",
    "    # Calculate AUC-ROC (One-vs-Rest)\n",
    "    auc_roc = roc_auc_score(test_y, test_probabilities, multi_class='ovr')\n",
    "    auc_roc_scores.append(auc_roc)\n",
    "    \n",
    "    # Calculate AUC-PR (One-vs-Rest)\n",
    "    auc_pr = average_precision_score(test_y, test_probabilities, average='weighted')\n",
    "    auc_pr_scores.append(auc_pr)\n",
    "    \n",
    "    # Calculate MCC\n",
    "    mcc = matthews_corrcoef(test_y, test_predictions)\n",
    "    mcc_scores.append(mcc)\n",
    "    \n",
    "    # Classification Report\n",
    "    report = classification_report(test_y, test_predictions, output_dict=True, zero_division=0)\n",
    "    \n",
    "    for class_label, metrics in report.items():\n",
    "        if isinstance(metrics, dict):  # Skip non-class metrics\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                class_metrics[class_label][metric_name] += metric_value\n",
    "\n",
    "# Calculate averages\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "average_f2_score = np.mean(f2_scores)\n",
    "average_auc_roc = np.mean(auc_roc_scores)\n",
    "average_auc_pr = np.mean(auc_pr_scores)\n",
    "average_mcc = np.mean(mcc_scores)\n",
    "\n",
    "# Average the classification report metrics\n",
    "average_class_metrics = {\n",
    "    class_label: {metric_name: metric_value / num_runs for metric_name, metric_value in metrics.items()}\n",
    "    for class_label, metrics in class_metrics.items()\n",
    "}\n",
    "\n",
    "# Output results\n",
    "print(f\"Average Test Accuracy: {average_accuracy:.4f}\")\n",
    "print(f\"Average F2-Score (Weighted): {average_f2_score:.4f}\")\n",
    "print(f\"Average AUC-ROC (One-vs-Rest): {average_auc_roc:.4f}\")\n",
    "print(f\"Average AUC-PR: {average_auc_pr:.4f}\")\n",
    "print(f\"Average Matthews Correlation Coefficient: {average_mcc:.4f}\")\n",
    "\n",
    "print(\"\\nAverage Classification Report:\")\n",
    "for class_label, metrics in average_class_metrics.items():\n",
    "    print(f\"Class {class_label}:\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"  {metric_name}: {metric_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Darknet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9989\n",
      "Classification Report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "Darknet_Audio-Streaming       1.00      1.00      1.00      2657\n",
      "       Darknet_Browsing       0.95      0.70      0.80        53\n",
      "           Darknet_Chat       1.00      1.00      1.00       908\n",
      "          Darknet_Email       1.00      0.99      1.00       116\n",
      "  Darknet_File-Transfer       1.00      1.00      1.00       522\n",
      "            Darknet_P2P       1.00      0.98      0.99        44\n",
      "           Darknet_VOIP       0.96      0.98      0.97       293\n",
      "Darknet_Video-Streaming       0.98      1.00      0.99       269\n",
      "                 Normal       1.00      1.00      1.00     26862\n",
      "\n",
      "               accuracy                           1.00     31724\n",
      "              macro avg       0.99      0.96      0.97     31724\n",
      "           weighted avg       1.00      1.00      1.00     31724\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the training dataset\n",
    "train_data = pd.read_csv(\"darknet_train.csv\")\n",
    "\n",
    "# Load the test dataset\n",
    "test_data = pd.read_csv(\"darknet_test.csv\")\n",
    "\n",
    "# Parameters\n",
    "num_runs = 5  # Number of iterations\n",
    "\n",
    "# Initialize accumulators\n",
    "accuracy_scores = []\n",
    "f2_scores = []\n",
    "auc_roc_scores = []\n",
    "auc_pr_scores = []\n",
    "mcc_scores = []\n",
    "\n",
    "# Initialize metrics accumulators for classification report\n",
    "class_metrics = defaultdict(lambda: defaultdict(float))  # Stores precision/recall/F1 for each class\n",
    "\n",
    "verbose_run = 1\n",
    "for run in range(num_runs):\n",
    "    print(f\"Run {run + 1}/{num_runs}\")\n",
    "    \n",
    "    # Shuffle data\n",
    "    train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "    test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Split features and labels\n",
    "    train_y = train_data['label'].values\n",
    "    train_X = train_data.drop(['label'], axis=1).values\n",
    "    test_y = test_data['label'].values\n",
    "    test_X = test_data.drop(['label'], axis=1).values\n",
    "    \n",
    "    # Train the model\n",
    "    clf = TabNetClassifier(verbose=verbose_run)\n",
    "    clf.fit(train_X, train_y, eval_set=[(train_X, train_y)], eval_metric=['logloss'])\n",
    "    verbose_run = 0\n",
    "    \n",
    "    # Make predictions\n",
    "    test_probabilities = clf.predict_proba(test_X)  # Outputs probabilities\n",
    "    test_predictions = clf.predict(test_X)  # Convert to discrete class predictions\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(test_y, test_predictions)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    \n",
    "    # Calculate F2-Score\n",
    "    f2_score = fbeta_score(test_y, test_predictions, beta=2, average='weighted')\n",
    "    f2_scores.append(f2_score)\n",
    "    \n",
    "    # Calculate AUC-ROC (One-vs-Rest)\n",
    "    auc_roc = roc_auc_score(test_y, test_probabilities, multi_class='ovr')\n",
    "    auc_roc_scores.append(auc_roc)\n",
    "    \n",
    "    # Calculate AUC-PR (One-vs-Rest)\n",
    "    auc_pr = average_precision_score(test_y, test_probabilities, average='weighted')\n",
    "    auc_pr_scores.append(auc_pr)\n",
    "    \n",
    "    # Calculate MCC\n",
    "    mcc = matthews_corrcoef(test_y, test_predictions)\n",
    "    mcc_scores.append(mcc)\n",
    "    \n",
    "    # Classification Report\n",
    "    report = classification_report(test_y, test_predictions, output_dict=True, zero_division=0)\n",
    "    \n",
    "    for class_label, metrics in report.items():\n",
    "        if isinstance(metrics, dict):  # Skip non-class metrics\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                class_metrics[class_label][metric_name] += metric_value\n",
    "\n",
    "# Calculate averages\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "average_f2_score = np.mean(f2_scores)\n",
    "average_auc_roc = np.mean(auc_roc_scores)\n",
    "average_auc_pr = np.mean(auc_pr_scores)\n",
    "average_mcc = np.mean(mcc_scores)\n",
    "\n",
    "# Average the classification report metrics\n",
    "average_class_metrics = {\n",
    "    class_label: {metric_name: metric_value / num_runs for metric_name, metric_value in metrics.items()}\n",
    "    for class_label, metrics in class_metrics.items()\n",
    "}\n",
    "\n",
    "# Output results\n",
    "print(f\"Average Test Accuracy: {average_accuracy:.4f}\")\n",
    "print(f\"Average F2-Score (Weighted): {average_f2_score:.4f}\")\n",
    "print(f\"Average AUC-ROC (One-vs-Rest): {average_auc_roc:.4f}\")\n",
    "print(f\"Average AUC-PR: {average_auc_pr:.4f}\")\n",
    "print(f\"Average Matthews Correlation Coefficient: {average_mcc:.4f}\")\n",
    "\n",
    "print(\"\\nAverage Classification Report:\")\n",
    "for class_label, metrics in average_class_metrics.items():\n",
    "    print(f\"Class {class_label}:\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"  {metric_name}: {metric_value:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
