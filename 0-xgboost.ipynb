{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuttle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/5\n",
      "Run 2/5\n",
      "Run 3/5\n",
      "Run 4/5\n",
      "Run 5/5\n",
      "Average Test Accuracy: 0.9997\n",
      "Average F2-Score (Weighted): 0.9997\n",
      "Average AUC-ROC (One-vs-Rest): 1.0000\n",
      "Average AUC-PR: 1.0000\n",
      "Average Matthews Correlation Coefficient: 0.9993\n",
      "\n",
      "Average Classification Report:\n",
      "Class 0:\n",
      "  precision: 0.9999\n",
      "  recall: 1.0000\n",
      "  f1-score: 0.9999\n",
      "  support: 9117.0000\n",
      "Class 1:\n",
      "  precision: 1.0000\n",
      "  recall: 1.0000\n",
      "  f1-score: 1.0000\n",
      "  support: 10.0000\n",
      "Class 2:\n",
      "  precision: 1.0000\n",
      "  recall: 0.9706\n",
      "  f1-score: 0.9851\n",
      "  support: 34.0000\n",
      "Class 3:\n",
      "  precision: 0.9994\n",
      "  recall: 0.9994\n",
      "  f1-score: 0.9994\n",
      "  support: 1781.0000\n",
      "Class 4:\n",
      "  precision: 0.9985\n",
      "  recall: 1.0000\n",
      "  f1-score: 0.9992\n",
      "  support: 653.0000\n",
      "Class 5:\n",
      "  precision: 1.0000\n",
      "  recall: 0.5000\n",
      "  f1-score: 0.6667\n",
      "  support: 2.0000\n",
      "Class 6:\n",
      "  precision: 1.0000\n",
      "  recall: 1.0000\n",
      "  f1-score: 1.0000\n",
      "  support: 3.0000\n",
      "Class macro avg:\n",
      "  precision: 0.9997\n",
      "  recall: 0.9243\n",
      "  f1-score: 0.9501\n",
      "  support: 11600.0000\n",
      "Class weighted avg:\n",
      "  precision: 0.9997\n",
      "  recall: 0.9997\n",
      "  f1-score: 0.9997\n",
      "  support: 11600.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import fbeta_score, roc_auc_score, average_precision_score, matthews_corrcoef\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the training dataset\n",
    "train_data = pd.read_csv(\"shuttle_train.csv\")\n",
    "train_data['label'] = train_data['label'] - 1  # Subtract 1 to make labels zero-based\n",
    "\n",
    "# Load the test dataset\n",
    "test_data = pd.read_csv(\"shuttle_test.csv\")\n",
    "test_data['label'] = test_data['label'] - 1  # Subtract 1 to make labels zero-based\n",
    "\n",
    "# Parameters\n",
    "num_runs = 5  # Number of iterations\n",
    "num_round = 100\n",
    "params = {\n",
    "    \"objective\": \"multi:softprob\",  # Output probabilities\n",
    "    \"num_class\": len(train_data['label'].unique()),  # Number of unique classes in the target\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"max_depth\": 6,\n",
    "    \"eta\": 0.3,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "# Initialize accumulators\n",
    "accuracy_scores = []\n",
    "f2_scores = []\n",
    "auc_roc_scores = []\n",
    "auc_pr_scores = []\n",
    "mcc_scores = []\n",
    "\n",
    "# Initialize metrics accumulators for classification report\n",
    "class_metrics = defaultdict(lambda: defaultdict(float))  # Stores precision/recall/F1 for each class\n",
    "\n",
    "for run in range(num_runs):\n",
    "    print(f\"Run {run + 1}/{num_runs}\")\n",
    "    \n",
    "    # Shuffle data\n",
    "    train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "    test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Replace infinities and fill NaN\n",
    "    train_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    train_data.fillna(train_data.mean(), inplace=True)\n",
    "    test_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    test_data.fillna(test_data.mean(), inplace=True)\n",
    "    \n",
    "    # Split features and labels\n",
    "    train_y = train_data['label']\n",
    "    train_X = train_data.drop(['label'], axis=1)\n",
    "    test_y = test_data['label']\n",
    "    test_X = test_data.drop(['label'], axis=1)\n",
    "    \n",
    "    # Convert to DMatrix for XGBoost\n",
    "    dtrain = xgb.DMatrix(data=train_X, label=train_y)\n",
    "    dtest = xgb.DMatrix(data=test_X, label=test_y)\n",
    "    \n",
    "    # Train the model\n",
    "    bst = xgb.train(params, dtrain, num_round)\n",
    "    \n",
    "    # Make predictions\n",
    "    test_probabilities = bst.predict(dtest)  # Outputs probabilities\n",
    "    test_predictions = np.argmax(test_probabilities, axis=1)  # Convert to discrete class predictions\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(test_y, test_predictions)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    \n",
    "    # Calculate F2-Score\n",
    "    f2_score = fbeta_score(test_y, test_predictions, beta=2, average='weighted')\n",
    "    f2_scores.append(f2_score)\n",
    "    \n",
    "    # Calculate AUC-ROC (One-vs-Rest)\n",
    "    auc_roc = roc_auc_score(test_y, test_probabilities, multi_class='ovr')\n",
    "    auc_roc_scores.append(auc_roc)\n",
    "    \n",
    "    # Calculate AUC-PR (One-vs-Rest)\n",
    "    auc_pr = average_precision_score(test_y, test_probabilities, average='weighted')\n",
    "    auc_pr_scores.append(auc_pr)\n",
    "    \n",
    "    # Calculate MCC\n",
    "    mcc = matthews_corrcoef(test_y, test_predictions)\n",
    "    mcc_scores.append(mcc)\n",
    "    \n",
    "    # Classification Report\n",
    "    report = classification_report(test_y, test_predictions, output_dict=True, zero_division=0)\n",
    "    \n",
    "    for class_label, metrics in report.items():\n",
    "        if isinstance(metrics, dict):  # Skip non-class metrics\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                class_metrics[class_label][metric_name] += metric_value\n",
    "\n",
    "# Calculate averages\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "average_f2_score = np.mean(f2_scores)\n",
    "average_auc_roc = np.mean(auc_roc_scores)\n",
    "average_auc_pr = np.mean(auc_pr_scores)\n",
    "average_mcc = np.mean(mcc_scores)\n",
    "\n",
    "# Average the classification report metrics\n",
    "average_class_metrics = {\n",
    "    class_label: {metric_name: metric_value / num_runs for metric_name, metric_value in metrics.items()}\n",
    "    for class_label, metrics in class_metrics.items()\n",
    "}\n",
    "\n",
    "# Output results\n",
    "print(f\"Average Test Accuracy: {average_accuracy:.4f}\")\n",
    "print(f\"Average F2-Score (Weighted): {average_f2_score:.4f}\")\n",
    "print(f\"Average AUC-ROC (One-vs-Rest): {average_auc_roc:.4f}\")\n",
    "print(f\"Average AUC-PR: {average_auc_pr:.4f}\")\n",
    "print(f\"Average Matthews Correlation Coefficient: {average_mcc:.4f}\")\n",
    "\n",
    "print(\"\\nAverage Classification Report:\")\n",
    "for class_label, metrics in average_class_metrics.items():\n",
    "    print(f\"Class {class_label}:\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"  {metric_name}: {metric_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runtime: 32s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covertype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/5\n",
      "Run 2/5\n",
      "Run 3/5\n",
      "Run 4/5\n",
      "Run 5/5\n",
      "Average Test Accuracy: 0.8696\n",
      "Average F2-Score (Weighted): 0.8692\n",
      "Average AUC-ROC (One-vs-Rest): 0.9863\n",
      "Average AUC-PR: 0.9457\n",
      "Average Matthews Correlation Coefficient: 0.7895\n",
      "\n",
      "Average Classification Report:\n",
      "Class 0:\n",
      "  precision: 0.8627\n",
      "  recall: 0.8407\n",
      "  f1-score: 0.8515\n",
      "  support: 42368.0000\n",
      "Class 1:\n",
      "  precision: 0.8671\n",
      "  recall: 0.8963\n",
      "  f1-score: 0.8815\n",
      "  support: 56661.0000\n",
      "Class 2:\n",
      "  precision: 0.8899\n",
      "  recall: 0.9095\n",
      "  f1-score: 0.8996\n",
      "  support: 7151.0000\n",
      "Class 3:\n",
      "  precision: 0.8787\n",
      "  recall: 0.8579\n",
      "  f1-score: 0.8682\n",
      "  support: 549.0000\n",
      "Class 4:\n",
      "  precision: 0.8891\n",
      "  recall: 0.6077\n",
      "  f1-score: 0.7219\n",
      "  support: 1899.0000\n",
      "Class 5:\n",
      "  precision: 0.8493\n",
      "  recall: 0.7970\n",
      "  f1-score: 0.8223\n",
      "  support: 3473.0000\n",
      "Class 6:\n",
      "  precision: 0.9508\n",
      "  recall: 0.9139\n",
      "  f1-score: 0.9320\n",
      "  support: 4102.0000\n",
      "Class macro avg:\n",
      "  precision: 0.8839\n",
      "  recall: 0.8319\n",
      "  f1-score: 0.8539\n",
      "  support: 116203.0000\n",
      "Class weighted avg:\n",
      "  precision: 0.8697\n",
      "  recall: 0.8696\n",
      "  f1-score: 0.8690\n",
      "  support: 116203.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import fbeta_score, roc_auc_score, average_precision_score, matthews_corrcoef\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the training dataset\n",
    "train_data = pd.read_csv(\"covtype_train.csv\")\n",
    "train_data['label'] = train_data['label'] - 1  # Subtract 1 to make labels zero-based\n",
    "\n",
    "# Load the test dataset\n",
    "test_data = pd.read_csv(\"covtype_test.csv\")\n",
    "test_data['label'] = test_data['label'] - 1  # Subtract 1 to make labels zero-based\n",
    "\n",
    "# Parameters\n",
    "num_runs = 5  # Number of iterations\n",
    "num_round = 100\n",
    "params = {\n",
    "    \"objective\": \"multi:softprob\",  # Output probabilities\n",
    "    \"num_class\": len(train_data['label'].unique()),  # Number of unique classes in the target\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"max_depth\": 6,\n",
    "    \"eta\": 0.3,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "# Initialize accumulators\n",
    "accuracy_scores = []\n",
    "f2_scores = []\n",
    "auc_roc_scores = []\n",
    "auc_pr_scores = []\n",
    "mcc_scores = []\n",
    "\n",
    "# Initialize metrics accumulators for classification report\n",
    "class_metrics = defaultdict(lambda: defaultdict(float))  # Stores precision/recall/F1 for each class\n",
    "\n",
    "for run in range(num_runs):\n",
    "    print(f\"Run {run + 1}/{num_runs}\")\n",
    "    \n",
    "    # Shuffle data\n",
    "    train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "    test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Replace infinities and fill NaN\n",
    "    train_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    train_data.fillna(train_data.mean(), inplace=True)\n",
    "    test_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    test_data.fillna(test_data.mean(), inplace=True)\n",
    "    \n",
    "    # Split features and labels\n",
    "    train_y = train_data['label']\n",
    "    train_X = train_data.drop(['label'], axis=1)\n",
    "    test_y = test_data['label']\n",
    "    test_X = test_data.drop(['label'], axis=1)\n",
    "    \n",
    "    # Convert to DMatrix for XGBoost\n",
    "    dtrain = xgb.DMatrix(data=train_X, label=train_y)\n",
    "    dtest = xgb.DMatrix(data=test_X, label=test_y)\n",
    "    \n",
    "    # Train the model\n",
    "    bst = xgb.train(params, dtrain, num_round)\n",
    "    \n",
    "    # Make predictions\n",
    "    test_probabilities = bst.predict(dtest)  # Outputs probabilities\n",
    "    test_predictions = np.argmax(test_probabilities, axis=1)  # Convert to discrete class predictions\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(test_y, test_predictions)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    \n",
    "    # Calculate F2-Score\n",
    "    f2_score = fbeta_score(test_y, test_predictions, beta=2, average='weighted')\n",
    "    f2_scores.append(f2_score)\n",
    "    \n",
    "    # Calculate AUC-ROC (One-vs-Rest)\n",
    "    auc_roc = roc_auc_score(test_y, test_probabilities, multi_class='ovr')\n",
    "    auc_roc_scores.append(auc_roc)\n",
    "    \n",
    "    # Calculate AUC-PR (One-vs-Rest)\n",
    "    auc_pr = average_precision_score(test_y, test_probabilities, average='weighted')\n",
    "    auc_pr_scores.append(auc_pr)\n",
    "    \n",
    "    # Calculate MCC\n",
    "    mcc = matthews_corrcoef(test_y, test_predictions)\n",
    "    mcc_scores.append(mcc)\n",
    "    \n",
    "    # Classification Report\n",
    "    report = classification_report(test_y, test_predictions, output_dict=True, zero_division=0)\n",
    "    \n",
    "    for class_label, metrics in report.items():\n",
    "        if isinstance(metrics, dict):  # Skip non-class metrics\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                class_metrics[class_label][metric_name] += metric_value\n",
    "\n",
    "# Calculate averages\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "average_f2_score = np.mean(f2_scores)\n",
    "average_auc_roc = np.mean(auc_roc_scores)\n",
    "average_auc_pr = np.mean(auc_pr_scores)\n",
    "average_mcc = np.mean(mcc_scores)\n",
    "\n",
    "# Average the classification report metrics\n",
    "average_class_metrics = {\n",
    "    class_label: {metric_name: metric_value / num_runs for metric_name, metric_value in metrics.items()}\n",
    "    for class_label, metrics in class_metrics.items()\n",
    "}\n",
    "\n",
    "# Output results\n",
    "print(f\"Average Test Accuracy: {average_accuracy:.4f}\")\n",
    "print(f\"Average F2-Score (Weighted): {average_f2_score:.4f}\")\n",
    "print(f\"Average AUC-ROC (One-vs-Rest): {average_auc_roc:.4f}\")\n",
    "print(f\"Average AUC-PR: {average_auc_pr:.4f}\")\n",
    "print(f\"Average Matthews Correlation Coefficient: {average_mcc:.4f}\")\n",
    "\n",
    "print(\"\\nAverage Classification Report:\")\n",
    "for class_label, metrics in average_class_metrics.items():\n",
    "    print(f\"Class {class_label}:\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"  {metric_name}: {metric_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runtime: 41.6s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/5\n",
      "Run 2/5\n",
      "Run 3/5\n",
      "Run 4/5\n",
      "Run 5/5\n",
      "Average Test Accuracy: 0.9998\n",
      "Average F2-Score (Weighted): 0.9998\n",
      "Average AUC-ROC (One-vs-Rest): 1.0000\n",
      "Average AUC-PR: 1.0000\n",
      "Average Matthews Correlation Coefficient: 0.9980\n",
      "\n",
      "Average Classification Report:\n",
      "Class 0:\n",
      "  precision: 0.9999\n",
      "  recall: 1.0000\n",
      "  f1-score: 0.9999\n",
      "  support: 194557.0000\n",
      "Class 1:\n",
      "  precision: 0.9987\n",
      "  recall: 0.9987\n",
      "  f1-score: 0.9987\n",
      "  support: 3178.0000\n",
      "Class 2:\n",
      "  precision: 0.9976\n",
      "  recall: 0.9996\n",
      "  f1-score: 0.9986\n",
      "  support: 2496.0000\n",
      "Class 3:\n",
      "  precision: 0.9995\n",
      "  recall: 0.9976\n",
      "  f1-score: 0.9986\n",
      "  support: 2083.0000\n",
      "Class 4:\n",
      "  precision: 1.0000\n",
      "  recall: 0.9935\n",
      "  f1-score: 0.9967\n",
      "  support: 463.0000\n",
      "Class 5:\n",
      "  precision: 0.9977\n",
      "  recall: 0.9977\n",
      "  f1-score: 0.9977\n",
      "  support: 441.0000\n",
      "Class 6:\n",
      "  precision: 0.9950\n",
      "  recall: 0.9755\n",
      "  f1-score: 0.9851\n",
      "  support: 204.0000\n",
      "Class 7:\n",
      "  precision: 1.0000\n",
      "  recall: 1.0000\n",
      "  f1-score: 1.0000\n",
      "  support: 196.0000\n",
      "Class 8:\n",
      "  precision: 1.0000\n",
      "  recall: 0.9811\n",
      "  f1-score: 0.9905\n",
      "  support: 53.0000\n",
      "Class 9:\n",
      "  precision: 1.0000\n",
      "  recall: 0.9091\n",
      "  f1-score: 0.9524\n",
      "  support: 11.0000\n",
      "Class 10:\n",
      "  precision: 1.0000\n",
      "  recall: 0.8333\n",
      "  f1-score: 0.9091\n",
      "  support: 6.0000\n",
      "Class 11:\n",
      "  precision: 1.0000\n",
      "  recall: 0.5000\n",
      "  f1-score: 0.6667\n",
      "  support: 4.0000\n",
      "Class 12:\n",
      "  precision: 1.0000\n",
      "  recall: 0.7500\n",
      "  f1-score: 0.8571\n",
      "  support: 4.0000\n",
      "Class 13:\n",
      "  precision: 1.0000\n",
      "  recall: 0.5000\n",
      "  f1-score: 0.6667\n",
      "  support: 2.0000\n",
      "Class 14:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 2.0000\n",
      "Class 15:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 2.0000\n",
      "Class 16:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 1.0000\n",
      "Class 17:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 2.0000\n",
      "Class 18:\n",
      "  precision: 1.0000\n",
      "  recall: 1.0000\n",
      "  f1-score: 1.0000\n",
      "  support: 1.0000\n",
      "Class 19:\n",
      "  precision: 1.0000\n",
      "  recall: 1.0000\n",
      "  f1-score: 1.0000\n",
      "  support: 1.0000\n",
      "Class 20:\n",
      "  precision: 1.0000\n",
      "  recall: 1.0000\n",
      "  f1-score: 1.0000\n",
      "  support: 1.0000\n",
      "Class macro avg:\n",
      "  precision: 0.8090\n",
      "  recall: 0.7351\n",
      "  f1-score: 0.7628\n",
      "  support: 203708.0000\n",
      "Class weighted avg:\n",
      "  precision: 0.9998\n",
      "  recall: 0.9998\n",
      "  f1-score: 0.9998\n",
      "  support: 203708.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import fbeta_score, roc_auc_score, average_precision_score, matthews_corrcoef\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define label mapping for KDD dataset labels\n",
    "label_mapping = {\n",
    "    'normal.': 0, 'satan.': 1, 'ipsweep.': 2, 'portsweep.': 3, 'nmap.': 4,\n",
    "    'back.': 5, 'warezclient.': 6, 'teardrop.': 7, 'pod.': 8, 'guess_passwd.': 9,\n",
    "    'buffer_overflow.': 10, 'land.': 11, 'warezmaster.': 12, 'imap.': 13, 'rootkit.': 14,\n",
    "    'loadmodule.': 15, 'multihop.': 16, 'ftp_write.': 17, 'phf.': 18, 'perl.': 19, 'spy.': 20\n",
    "}\n",
    "\n",
    "# Load the training dataset\n",
    "train_data = pd.read_csv(\"kdd_train.csv\")\n",
    "train_data['label'] = train_data['label'].map(label_mapping)  # Convert labels to numeric values\n",
    "\n",
    "# Load the test dataset\n",
    "test_data = pd.read_csv(\"kdd_test.csv\")\n",
    "test_data['label'] = test_data['label'].map(label_mapping)  # Convert labels to numeric values\n",
    "\n",
    "# Parameters\n",
    "num_runs = 5  # Number of iterations\n",
    "num_round = 100\n",
    "params = {\n",
    "    \"objective\": \"multi:softprob\",  # Output probabilities\n",
    "    \"num_class\": len(train_data['label'].unique()),  # Number of unique classes in the target\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"max_depth\": 6,\n",
    "    \"eta\": 0.3,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "# Initialize accumulators\n",
    "accuracy_scores = []\n",
    "f2_scores = []\n",
    "auc_roc_scores = []\n",
    "auc_pr_scores = []\n",
    "mcc_scores = []\n",
    "\n",
    "# Initialize metrics accumulators for classification report\n",
    "class_metrics = defaultdict(lambda: defaultdict(float))  # Stores precision/recall/F1 for each class\n",
    "\n",
    "for run in range(num_runs):\n",
    "    print(f\"Run {run + 1}/{num_runs}\")\n",
    "    \n",
    "    # Shuffle data\n",
    "    train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "    test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Replace infinities and fill NaN\n",
    "    train_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    train_data.fillna(train_data.mean(), inplace=True)\n",
    "    test_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    test_data.fillna(test_data.mean(), inplace=True)\n",
    "    \n",
    "    # Split features and labels\n",
    "    train_y = train_data['label']\n",
    "    train_X = train_data.drop(['label'], axis=1)\n",
    "    test_y = test_data['label']\n",
    "    test_X = test_data.drop(['label'], axis=1)\n",
    "    \n",
    "    # Convert to DMatrix for XGBoost\n",
    "    dtrain = xgb.DMatrix(data=train_X, label=train_y)\n",
    "    dtest = xgb.DMatrix(data=test_X, label=test_y)\n",
    "    \n",
    "    # Train the model\n",
    "    bst = xgb.train(params, dtrain, num_round)\n",
    "    \n",
    "    # Make predictions\n",
    "    test_probabilities = bst.predict(dtest)  # Outputs probabilities\n",
    "    test_predictions = np.argmax(test_probabilities, axis=1)  # Convert to discrete class predictions\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(test_y, test_predictions)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    \n",
    "    # Calculate F2-Score\n",
    "    f2_score = fbeta_score(test_y, test_predictions, beta=2, average='weighted')\n",
    "    f2_scores.append(f2_score)\n",
    "    \n",
    "    # Calculate AUC-ROC (One-vs-Rest)\n",
    "    auc_roc = roc_auc_score(test_y, test_probabilities, multi_class='ovr')\n",
    "    auc_roc_scores.append(auc_roc)\n",
    "    \n",
    "    # Calculate AUC-PR (One-vs-Rest)\n",
    "    auc_pr = average_precision_score(test_y, test_probabilities, average='weighted')\n",
    "    auc_pr_scores.append(auc_pr)\n",
    "    \n",
    "    # Calculate MCC\n",
    "    mcc = matthews_corrcoef(test_y, test_predictions)\n",
    "    mcc_scores.append(mcc)\n",
    "    \n",
    "    # Classification Report\n",
    "    report = classification_report(test_y, test_predictions, output_dict=True, zero_division=0)\n",
    "    \n",
    "    for class_label, metrics in report.items():\n",
    "        if isinstance(metrics, dict):  # Skip non-class metrics\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                class_metrics[class_label][metric_name] += metric_value\n",
    "\n",
    "# Calculate averages\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "average_f2_score = np.mean(f2_scores)\n",
    "average_auc_roc = np.mean(auc_roc_scores)\n",
    "average_auc_pr = np.mean(auc_pr_scores)\n",
    "average_mcc = np.mean(mcc_scores)\n",
    "\n",
    "# Average the classification report metrics\n",
    "average_class_metrics = {\n",
    "    class_label: {metric_name: metric_value / num_runs for metric_name, metric_value in metrics.items()}\n",
    "    for class_label, metrics in class_metrics.items()\n",
    "}\n",
    "\n",
    "# Output results\n",
    "print(f\"Average Test Accuracy: {average_accuracy:.4f}\")\n",
    "print(f\"Average F2-Score (Weighted): {average_f2_score:.4f}\")\n",
    "print(f\"Average AUC-ROC (One-vs-Rest): {average_auc_roc:.4f}\")\n",
    "print(f\"Average AUC-PR: {average_auc_pr:.4f}\")\n",
    "print(f\"Average Matthews Correlation Coefficient: {average_mcc:.4f}\")\n",
    "\n",
    "print(\"\\nAverage Classification Report:\")\n",
    "for class_label, metrics in average_class_metrics.items():\n",
    "    print(f\"Class {class_label}:\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"  {metric_name}: {metric_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runtime: 3m 37.5s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Darknet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/5\n",
      "Run 2/5\n",
      "Run 3/5\n",
      "Run 4/5\n",
      "Run 5/5\n",
      "Average Test Accuracy: 0.9997\n",
      "Average F2-Score (Weighted): 0.9997\n",
      "Average AUC-ROC (One-vs-Rest): 1.0000\n",
      "Average AUC-PR: 0.9999\n",
      "Average Matthews Correlation Coefficient: 0.9991\n",
      "\n",
      "Average Classification Report:\n",
      "Class 0:\n",
      "  precision: 0.9999\n",
      "  recall: 1.0000\n",
      "  f1-score: 0.9999\n",
      "  support: 26862.0000\n",
      "Class 1:\n",
      "  precision: 0.9998\n",
      "  recall: 1.0000\n",
      "  f1-score: 0.9999\n",
      "  support: 2657.0000\n",
      "Class 2:\n",
      "  precision: 1.0000\n",
      "  recall: 0.9978\n",
      "  f1-score: 0.9989\n",
      "  support: 908.0000\n",
      "Class 3:\n",
      "  precision: 1.0000\n",
      "  recall: 0.9977\n",
      "  f1-score: 0.9988\n",
      "  support: 522.0000\n",
      "Class 4:\n",
      "  precision: 0.9932\n",
      "  recall: 1.0000\n",
      "  f1-score: 0.9966\n",
      "  support: 293.0000\n",
      "Class 5:\n",
      "  precision: 1.0000\n",
      "  recall: 1.0000\n",
      "  f1-score: 1.0000\n",
      "  support: 269.0000\n",
      "Class 6:\n",
      "  precision: 1.0000\n",
      "  recall: 1.0000\n",
      "  f1-score: 1.0000\n",
      "  support: 116.0000\n",
      "Class 7:\n",
      "  precision: 0.9600\n",
      "  recall: 0.9057\n",
      "  f1-score: 0.9320\n",
      "  support: 53.0000\n",
      "Class 8:\n",
      "  precision: 1.0000\n",
      "  recall: 1.0000\n",
      "  f1-score: 1.0000\n",
      "  support: 44.0000\n",
      "Class macro avg:\n",
      "  precision: 0.9948\n",
      "  recall: 0.9890\n",
      "  f1-score: 0.9918\n",
      "  support: 31724.0000\n",
      "Class weighted avg:\n",
      "  precision: 0.9997\n",
      "  recall: 0.9997\n",
      "  f1-score: 0.9997\n",
      "  support: 31724.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import fbeta_score, roc_auc_score, average_precision_score, matthews_corrcoef\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define label mapping for Darknet dataset labels\n",
    "label_mapping = {\n",
    "    'Normal': 0, 'Darknet_Audio-Streaming': 1, 'Darknet_Chat': 2, 'Darknet_File-Transfer': 3, 'Darknet_VOIP': 4,\n",
    "    'Darknet_Video-Streaming': 5, 'Darknet_Email': 6, 'Darknet_Browsing': 7, 'Darknet_P2P': 8\n",
    "}\n",
    "\n",
    "# Load the training dataset\n",
    "train_data = pd.read_csv(\"darknet_train.csv\")\n",
    "train_data['label'] = train_data['label'].map(label_mapping)  # Convert labels to numeric values\n",
    "\n",
    "# Load the test dataset\n",
    "test_data = pd.read_csv(\"darknet_test.csv\")\n",
    "test_data['label'] = test_data['label'].map(label_mapping)  # Convert labels to numeric values\n",
    "\n",
    "# Parameters\n",
    "num_runs = 5  # Number of iterations\n",
    "num_round = 100\n",
    "params = {\n",
    "    \"objective\": \"multi:softprob\",  # Output probabilities\n",
    "    \"num_class\": len(train_data['label'].unique()),  # Number of unique classes in the target\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"max_depth\": 6,\n",
    "    \"eta\": 0.3,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "# Initialize accumulators\n",
    "accuracy_scores = []\n",
    "f2_scores = []\n",
    "auc_roc_scores = []\n",
    "auc_pr_scores = []\n",
    "mcc_scores = []\n",
    "\n",
    "# Initialize metrics accumulators for classification report\n",
    "class_metrics = defaultdict(lambda: defaultdict(float))  # Stores precision/recall/F1 for each class\n",
    "\n",
    "for run in range(num_runs):\n",
    "    print(f\"Run {run + 1}/{num_runs}\")\n",
    "    \n",
    "    # Shuffle data\n",
    "    train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "    test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Replace infinities and fill NaN\n",
    "    train_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    train_data.fillna(train_data.mean(), inplace=True)\n",
    "    test_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    test_data.fillna(test_data.mean(), inplace=True)\n",
    "    \n",
    "    # Split features and labels\n",
    "    train_y = train_data['label']\n",
    "    train_X = train_data.drop(['label'], axis=1)\n",
    "    test_y = test_data['label']\n",
    "    test_X = test_data.drop(['label'], axis=1)\n",
    "    \n",
    "    # Convert to DMatrix for XGBoost\n",
    "    dtrain = xgb.DMatrix(data=train_X, label=train_y)\n",
    "    dtest = xgb.DMatrix(data=test_X, label=test_y)\n",
    "    \n",
    "    # Train the model\n",
    "    bst = xgb.train(params, dtrain, num_round)\n",
    "    \n",
    "    # Make predictions\n",
    "    test_probabilities = bst.predict(dtest)  # Outputs probabilities\n",
    "    test_predictions = np.argmax(test_probabilities, axis=1)  # Convert to discrete class predictions\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(test_y, test_predictions)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    \n",
    "    # Calculate F2-Score\n",
    "    f2_score = fbeta_score(test_y, test_predictions, beta=2, average='weighted')\n",
    "    f2_scores.append(f2_score)\n",
    "    \n",
    "    # Calculate AUC-ROC (One-vs-Rest)\n",
    "    auc_roc = roc_auc_score(test_y, test_probabilities, multi_class='ovr')\n",
    "    auc_roc_scores.append(auc_roc)\n",
    "    \n",
    "    # Calculate AUC-PR (One-vs-Rest)\n",
    "    auc_pr = average_precision_score(test_y, test_probabilities, average='weighted')\n",
    "    auc_pr_scores.append(auc_pr)\n",
    "    \n",
    "    # Calculate MCC\n",
    "    mcc = matthews_corrcoef(test_y, test_predictions)\n",
    "    mcc_scores.append(mcc)\n",
    "    \n",
    "    # Classification Report\n",
    "    report = classification_report(test_y, test_predictions, output_dict=True, zero_division=0)\n",
    "    \n",
    "    for class_label, metrics in report.items():\n",
    "        if isinstance(metrics, dict):  # Skip non-class metrics\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                class_metrics[class_label][metric_name] += metric_value\n",
    "\n",
    "# Calculate averages\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "average_f2_score = np.mean(f2_scores)\n",
    "average_auc_roc = np.mean(auc_roc_scores)\n",
    "average_auc_pr = np.mean(auc_pr_scores)\n",
    "average_mcc = np.mean(mcc_scores)\n",
    "\n",
    "# Average the classification report metrics\n",
    "average_class_metrics = {\n",
    "    class_label: {metric_name: metric_value / num_runs for metric_name, metric_value in metrics.items()}\n",
    "    for class_label, metrics in class_metrics.items()\n",
    "}\n",
    "\n",
    "# Output results\n",
    "print(f\"Average Test Accuracy: {average_accuracy:.4f}\")\n",
    "print(f\"Average F2-Score (Weighted): {average_f2_score:.4f}\")\n",
    "print(f\"Average AUC-ROC (One-vs-Rest): {average_auc_roc:.4f}\")\n",
    "print(f\"Average AUC-PR: {average_auc_pr:.4f}\")\n",
    "print(f\"Average Matthews Correlation Coefficient: {average_mcc:.4f}\")\n",
    "\n",
    "print(\"\\nAverage Classification Report:\")\n",
    "for class_label, metrics in average_class_metrics.items():\n",
    "    print(f\"Class {class_label}:\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"  {metric_name}: {metric_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runtime: 20.4s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
