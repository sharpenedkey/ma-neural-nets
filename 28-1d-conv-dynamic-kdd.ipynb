{"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10123278,"sourceType":"datasetVersion","datasetId":6246813}],"dockerImageVersionId":30804,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":null,"end_time":null,"environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-12-15T20:58:20.961289","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"bddb84ed-c383-4478-9ed8-42e8e01f8667","_cell_guid":"0ef29cec-6934-41ab-856d-4255c513cd80","trusted":true,"collapsed":false,"execution":{"iopub.execute_input":"2024-12-15T20:58:24.415601Z","iopub.status.busy":"2024-12-15T20:58:24.414926Z","iopub.status.idle":"2024-12-15T20:58:24.428734Z","shell.execute_reply":"2024-12-15T20:58:24.427223Z"},"papermill":{"duration":0.023478,"end_time":"2024-12-15T20:58:24.432003","exception":false,"start_time":"2024-12-15T20:58:24.408525","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom collections import defaultdict\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import classification_report, balanced_accuracy_score, roc_auc_score, average_precision_score, fbeta_score, matthews_corrcoef\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import mixed_precision\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Conv1D, Flatten, BatchNormalization, Lambda","metadata":{"_uuid":"86e48948-ae84-4b7c-a22e-bf2f997efb28","_cell_guid":"0cb36ef8-85c2-4a39-88f0-2481405bc890","trusted":true,"collapsed":false,"execution":{"iopub.execute_input":"2024-12-15T20:58:24.440751Z","iopub.status.busy":"2024-12-15T20:58:24.440088Z","iopub.status.idle":"2024-12-15T20:58:42.357475Z","shell.execute_reply":"2024-12-15T20:58:42.356149Z"},"papermill":{"duration":17.924968,"end_time":"2024-12-15T20:58:42.360559","exception":false,"start_time":"2024-12-15T20:58:24.435591","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prevent TensorFlow from allocating all GPU memory at once\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)\n\n# Use mixed-precision training\npolicy = mixed_precision.Policy('mixed_float16')  # Automatically use FP16 where possible\nmixed_precision.set_global_policy(policy)","metadata":{"_uuid":"d27f38f0-3613-4eb1-8346-4cd694549141","_cell_guid":"45f07c83-54d9-4bbc-b61a-84942849dfe0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\n\ndef import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n    df = reduce_mem_usage(df)\n    return df","metadata":{"_uuid":"f99efd02-1971-4f53-b4f3-d3e51e29b6d2","_cell_guid":"008f0112-175b-4041-b483-3bc10f9471ac","trusted":true,"collapsed":false,"execution":{"iopub.execute_input":"2024-12-15T20:58:42.370911Z","iopub.status.busy":"2024-12-15T20:58:42.370167Z","iopub.status.idle":"2024-12-15T20:58:42.383889Z","shell.execute_reply":"2024-12-15T20:58:42.382676Z"},"papermill":{"duration":0.021303,"end_time":"2024-12-15T20:58:42.386299","exception":false,"start_time":"2024-12-15T20:58:42.364996","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_dynamic_cnn(input_dim, num_classes, sliding_window_size=3):\n    \"\"\"\n    Creates a CNN dynamically based on input dimensions and label classes,\n    with a decreasing number of filters in successive layers.\n    \"\"\"\n    # Input layer\n    input_layer = Input(shape=(input_dim,))\n    \n    # Reshape input to be compatible with Conv1D using Lambda layer\n    reshaped_input = Lambda(lambda x: tf.expand_dims(x, axis=-1))(input_layer)  # Shape: (batch_size, input_dim, 1)\n    \n    # Determine number of convolutional layers based on input size\n    num_conv_layers = max(2, input_dim // 10)  # At least 2 layers\n    filters = input_dim * 2  # Start with a reasonable number of filters\n    x = reshaped_input\n\n    # Add convolutional layers with decreasing filter counts\n    for _ in range(num_conv_layers):\n        x = Conv1D(filters=filters, kernel_size=sliding_window_size, activation='relu', padding='same', strides=1)(x)\n        x = BatchNormalization()(x)\n        x = Dropout(0.2)(x)\n        filters = int(max(16, filters // 2))  # Gradually decrease filters, but not below 16\n    \n    # Flatten the output of the final Conv1D layer\n    x = Flatten()(x)\n    \n    # Fully connected layers\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.2)(x)\n    output_layer = Dense(num_classes, activation='softmax')(x)\n    \n    # Create model\n    model = Model(inputs=input_layer, outputs=output_layer)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","metadata":{"_uuid":"44ce4895-180f-4765-a114-c7b9dd2df64f","_cell_guid":"4562ba62-e12b-4a93-bef4-36bdd310f2fd","trusted":true,"collapsed":false,"execution":{"iopub.execute_input":"2024-12-15T20:58:42.395619Z","iopub.status.busy":"2024-12-15T20:58:42.394706Z","iopub.status.idle":"2024-12-15T20:58:42.403830Z","shell.execute_reply":"2024-12-15T20:58:42.402494Z"},"papermill":{"duration":0.016221,"end_time":"2024-12-15T20:58:42.405958","exception":false,"start_time":"2024-12-15T20:58:42.389737","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_model(train_data_path, test_data_path, is_string_labels = False, label_mapping = None):\n\n    # Initialize the one-hot encoder for the target\n    encoder = OneHotEncoder(sparse_output=False)\n\n    # Load and Prepare Training Data\n    train_data = import_data(train_data_path)\n    train_data = train_data.sample(frac=1).reset_index(drop=True)  # Shuffle\n    if (is_string_labels):\n        train_data['label'] = train_data['label'].map(label_mapping)\n    train_X = train_data.drop(columns=['label']).values\n    train_y = train_data['label'].values\n    train_y = encoder.fit_transform(train_y.reshape(-1, 1))\n    del train_data\n    gc.collect()\n\n    # Load and Prepare Test Data (this will not be used in training)\n    test_data = import_data(test_data_path)\n    test_data = test_data.sample(frac=1).reset_index(drop=True)  # Shuffle\n    if (is_string_labels):\n        test_data['label'] = test_data['label'].map(label_mapping)\n    test_X = test_data.drop(columns=['label']).values\n    test_y = test_data['label'].values\n    test_y = encoder.transform(test_y.reshape(-1, 1))\n    del test_data\n    gc.collect()\n\n    # Create tf.data datasets\n    batch_size = int(train_X.shape[0] * 0.01)\n    train_dataset = tf.data.Dataset.from_tensor_slices((train_X, train_y))\n    train_dataset = train_dataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n    test_dataset = tf.data.Dataset.from_tensor_slices((test_X, test_y))\n    test_dataset = test_dataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n\n    # EarlyStopping Callback (optional, to avoid overfitting)\n    early_stopping = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n\n    # Number of runs for averaging results\n    num_runs = 5\n\n    # Initialize storage for metrics\n    metrics_storage = defaultdict(list)\n\n    # Perform a warm-up run to initialize TensorFlow and the GPU\n    warmup_X = np.random.rand(10, train_X.shape[1]).astype(np.float32)\n    warmup_y = np.random.rand(10, train_y.shape[1]).astype(np.float32)\n\n    model = create_dynamic_cnn(train_X.shape[1], train_y.shape[1])\n    model.fit(warmup_X, warmup_y, epochs=1, batch_size=1, verbose=0)\n\n    # Train the Model with Validation Split N tines for more accurate metrics\n    print(\"Verbose output only for first run...\")\n    verbose_run = 1\n    for run in range(num_runs):\n        \n        model = create_dynamic_cnn(train_X.shape[1], train_y.shape[1])\n\n        print(f\"Run {run + 1}/{num_runs} started...\")\n        history = model.fit(\n            train_dataset,\n            epochs=100, \n            callbacks=[early_stopping],\n            verbose=verbose_run\n        )\n        verbose_run = 0 # Suppress detailed output for multiple runs\n\n        test_loss, test_acc = model.evaluate(test_dataset, verbose=0)\n        y_pred = model.predict(test_dataset, verbose=0)\n        y_true = np.concatenate([y.numpy() for _, y in test_dataset], axis=0)\n        y_pred_classes = y_pred.argmax(axis=1)\n        y_true_classes = y_true.argmax(axis=1)\n\n        # Compute metrics\n        balanced_acc = balanced_accuracy_score(y_true_classes, y_pred_classes)\n        roc_auc = roc_auc_score(y_true, y_pred, multi_class='ovr')  # `test_y` is fine here for AUC\n        pr_auc = average_precision_score(y_true, y_pred, average='weighted')\n        f2 = fbeta_score(y_true_classes, y_pred_classes, beta=2, average='weighted')\n        mcc = matthews_corrcoef(y_true_classes, y_pred_classes)\n\n        # Store metrics\n        metrics_storage['test_loss'].append(test_loss)\n        metrics_storage['test_accuracy'].append(test_acc)\n        metrics_storage['balanced_accuracy'].append(balanced_acc)\n        metrics_storage['roc_auc'].append(roc_auc)\n        metrics_storage['pr_auc'].append(pr_auc)\n        metrics_storage['f2'].append(f2)\n        metrics_storage['mcc'].append(mcc)\n\n        # Store classification report metrics\n        report = classification_report(y_true_classes, y_pred_classes, output_dict=True)\n        for label, values in report.items():\n            # Check if the value is a dictionary (e.g., 'precision', 'recall', 'f1-score')\n            if isinstance(values, dict):\n                for metric, value in values.items():\n                    metrics_storage[f\"{label}_{metric}\"].append(value)\n            else:\n                # Handle scalar values (like 'accuracy')\n                metrics_storage[label].append(values)\n\n        # Average the metrics over all successful runs\n        print(f\"\\nAggregated Metrics for {run+1} runs:\")\n        for metric, values in metrics_storage.items():\n            avg_value = np.mean(values)\n            print(f\"{metric}: {avg_value:.4f}\")\n\n    gc.collect()","metadata":{"_uuid":"ab45b2ad-c87e-40a4-ba89-a416f44f1252","_cell_guid":"2302e882-9510-4e82-ba6e-9ef7ca4fed4c","trusted":true,"collapsed":false,"execution":{"iopub.execute_input":"2024-12-15T20:58:42.414411Z","iopub.status.busy":"2024-12-15T20:58:42.413902Z","iopub.status.idle":"2024-12-15T20:58:42.430723Z","shell.execute_reply":"2024-12-15T20:58:42.429317Z"},"papermill":{"duration":0.024591,"end_time":"2024-12-15T20:58:42.433560","exception":false,"start_time":"2024-12-15T20:58:42.408969","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels_map = {\n    'normal.': 0, 'satan.': 1, 'ipsweep.': 2, 'portsweep.': 3, 'nmap.': 4,\n    'back.': 5, 'warezclient.': 6, 'teardrop.': 7, 'pod.': 8, 'guess_passwd.': 9,\n    'buffer_overflow.': 10, 'land.': 11, 'warezmaster.': 12, 'imap.': 13, 'rootkit.': 14,\n    'loadmodule.': 15, 'multihop.': 16, 'ftp_write.': 17, 'phf.': 18, 'perl.': 19, 'spy.': 20\n}\n\nrun_model(\"/kaggle/input/ma-datasets/kdd_train.csv\", \"/kaggle/input/ma-datasets/kdd_test.csv\", is_string_labels = True, label_mapping=labels_map)","metadata":{"_uuid":"5c2dfebc-ecea-4e1d-aad7-01c98dc4cc04","_cell_guid":"064f77ec-58ea-4d44-ac6b-9bcbf991ae3a","trusted":true,"collapsed":false,"execution":{"iopub.execute_input":"2024-12-16T05:02:24.866116Z","iopub.status.busy":"2024-12-16T05:02:24.865440Z"},"papermill":{"duration":null,"end_time":null,"exception":false,"start_time":"2024-12-16T05:02:24.860161","status":"running"},"tags":[],"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}