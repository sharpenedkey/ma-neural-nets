{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d63a686c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-23T21:35:17.453197Z",
     "iopub.status.busy": "2024-12-23T21:35:17.452832Z",
     "iopub.status.idle": "2024-12-23T21:35:17.458455Z",
     "shell.execute_reply": "2024-12-23T21:35:17.457509Z"
    },
    "papermill": {
     "duration": 0.011515,
     "end_time": "2024-12-23T21:35:17.460236",
     "exception": false,
     "start_time": "2024-12-23T21:35:17.448721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c15c08b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T21:35:17.466541Z",
     "iopub.status.busy": "2024-12-23T21:35:17.466168Z",
     "iopub.status.idle": "2024-12-23T21:35:28.920689Z",
     "shell.execute_reply": "2024-12-23T21:35:28.919623Z"
    },
    "papermill": {
     "duration": 11.459598,
     "end_time": "2024-12-23T21:35:28.922654",
     "exception": false,
     "start_time": "2024-12-23T21:35:17.463056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import OneHotEncoder, label_binarize\n",
    "from sklearn.metrics import accuracy_score, classification_report, balanced_accuracy_score, roc_auc_score, average_precision_score, fbeta_score, matthews_corrcoef\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LayerNormalization, Input, Conv1D, Lambda, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5787ea14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T21:35:28.929423Z",
     "iopub.status.busy": "2024-12-23T21:35:28.928761Z",
     "iopub.status.idle": "2024-12-23T21:35:28.941968Z",
     "shell.execute_reply": "2024-12-23T21:35:28.940895Z"
    },
    "papermill": {
     "duration": 0.018149,
     "end_time": "2024-12-23T21:35:28.943576",
     "exception": false,
     "start_time": "2024-12-23T21:35:28.925427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def import_data(file):\n",
    "    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n",
    "    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92c7835f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T21:35:28.949549Z",
     "iopub.status.busy": "2024-12-23T21:35:28.949204Z",
     "iopub.status.idle": "2024-12-23T21:35:28.956612Z",
     "shell.execute_reply": "2024-12-23T21:35:28.955695Z"
    },
    "papermill": {
     "duration": 0.012124,
     "end_time": "2024-12-23T21:35:28.958224",
     "exception": false,
     "start_time": "2024-12-23T21:35:28.946100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_convnext_like_feature_extractor(input_dim, sliding_window_size=3):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    reshaped_input = Lambda(lambda x: tf.expand_dims(x, axis=-1))(input_layer)\n",
    "    x = reshaped_input\n",
    "\n",
    "    # Simulated ConvNeXt block\n",
    "    x = Conv1D(filters=int(max(16, input_dim * 2)), kernel_size=sliding_window_size, padding='same', activation='gelu', strides=1)(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Conv1D(filters=int(max(16, input_dim)), kernel_size=sliding_window_size, padding='same', activation='gelu', strides=1)(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Conv1D(filters=int(max(16, input_dim // 2)), kernel_size=sliding_window_size, padding='same', activation='gelu', strides=1)(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Conv1D(filters=int(max(16, input_dim // 4)), kernel_size=sliding_window_size, padding='same', activation='gelu', strides=1)(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Conv1D(filters=int(max(16, input_dim // 8)), kernel_size=sliding_window_size, padding='same', activation='gelu', strides=1)(x)\n",
    "    x = LayerNormalization()(x)\n",
    "\n",
    "    # Flatten the output for feature extraction\n",
    "    feature_output = Flatten()(x)\n",
    "\n",
    "    # Reconstruction head for self-supervised learning\n",
    "    reconstruction_output = Dense(input_dim, activation='linear')(feature_output)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=input_layer, outputs=reconstruction_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a0c10cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T21:35:28.964338Z",
     "iopub.status.busy": "2024-12-23T21:35:28.963992Z",
     "iopub.status.idle": "2024-12-23T21:35:28.977016Z",
     "shell.execute_reply": "2024-12-23T21:35:28.975993Z"
    },
    "papermill": {
     "duration": 0.017986,
     "end_time": "2024-12-23T21:35:28.978634",
     "exception": false,
     "start_time": "2024-12-23T21:35:28.960648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_model(train_data_path, test_data_path, is_string_labels = False, label_mapping = None):\n",
    "\n",
    "    # Load and Prepare Training Data\n",
    "    train_data = pd.read_csv(train_data_path)\n",
    "    train_data = train_data.sample(frac=1).reset_index(drop=True)  # Shuffle\n",
    "    if (is_string_labels):\n",
    "        train_data['label'] = train_data['label'].map(label_mapping)\n",
    "    train_X = train_data.drop(['label'], axis=1)\n",
    "    train_y = train_data['label'] - 1\n",
    "    del train_data\n",
    "    gc.collect()\n",
    "\n",
    "    # Load and Prepare Test Data (this will not be used in training)\n",
    "    test_data = pd.read_csv(test_data_path)\n",
    "    test_data = test_data.sample(frac=1).reset_index(drop=True)  # Shuffle\n",
    "    if (is_string_labels):\n",
    "        test_data['label'] = test_data['label'].map(label_mapping)\n",
    "    test_X = test_data.drop(['label'], axis=1)\n",
    "    test_y = test_data['label'] - 1\n",
    "    del test_data\n",
    "    gc.collect()\n",
    "\n",
    "    # Number of runs for averaging results\n",
    "    num_runs = 50\n",
    "\n",
    "    # Initialize storage for metrics\n",
    "    metrics_storage = defaultdict(list)\n",
    "\n",
    "    # Define XGBoost parameters\n",
    "    params = {\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"num_class\": len(train_y.unique()),\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"max_depth\": 6,\n",
    "        \"eta\": 0.3,\n",
    "        \"seed\": 42\n",
    "    }\n",
    "\n",
    "    # Train the Model with Validation Split N tines for more accurate metrics\n",
    "    #print(\"Verbose output only for first run...\")\n",
    "    verbose_run = 0\n",
    "    for run in range(num_runs):\n",
    "        print(f\"Run {run + 1}/{num_runs} started...\")\n",
    "                \n",
    "        # Train the NN feature extractor\n",
    "        feature_extractor = create_convnext_like_feature_extractor(train_X.shape[1])\n",
    "        feature_extractor.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        feature_extractor.fit(train_X, train_X, epochs=3, batch_size=32, verbose=0)  # Self-supervised training\n",
    "\n",
    "        # Extract features\n",
    "        feature_extractor_truncated = Model(inputs=feature_extractor.input, outputs=feature_extractor.layers[-2].output)\n",
    "        train_features = feature_extractor_truncated.predict(train_X, verbose=0)\n",
    "        test_features = feature_extractor_truncated.predict(test_X, verbose=0)\n",
    "\n",
    "        # Convert data to DMatrix\n",
    "        dtrain = xgb.DMatrix(data=train_features, label=train_y)\n",
    "        dtest = xgb.DMatrix(data=test_features, label=test_y)\n",
    "\n",
    "        # Train XGBoost\n",
    "        num_round = 100\n",
    "        bst = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "        # Make predictions\n",
    "        test_probabilities = bst.predict(dtest)\n",
    "        test_predictions = np.argmax(test_probabilities, axis=1)\n",
    "        del feature_extractor\n",
    "        gc.collect()\n",
    "\n",
    "        test_y_binarized = label_binarize(test_y, classes=range(len(np.unique(test_y))))\n",
    "\n",
    "        # Compute metrics\n",
    "        accuracy = accuracy_score(test_y, test_predictions)\n",
    "        balanced_acc = balanced_accuracy_score(test_y, test_predictions)\n",
    "        roc_auc = roc_auc_score(test_y, test_probabilities, multi_class='ovr')  # `test_y` is fine here for AUC\n",
    "        pr_auc_scores = []\n",
    "        for i in range(test_y_binarized.shape[1]):  # Iterate over each class\n",
    "            pr_auc = average_precision_score(test_y_binarized[:, i], test_probabilities[:, i])\n",
    "            pr_auc_scores.append(pr_auc)\n",
    "        pr_auc = np.mean(pr_auc_scores)\n",
    "        f2 = fbeta_score(test_y, test_predictions, beta=2, average='weighted')\n",
    "        mcc = matthews_corrcoef(test_y, test_predictions)\n",
    "\n",
    "        # Store metrics\n",
    "        metrics_storage['test_accuracy'].append(accuracy)\n",
    "        metrics_storage['balanced_accuracy'].append(balanced_acc)\n",
    "        metrics_storage['roc_auc'].append(roc_auc)\n",
    "        metrics_storage['pr_auc'].append(pr_auc)\n",
    "        metrics_storage['f2'].append(f2)\n",
    "        metrics_storage['mcc'].append(mcc)\n",
    "\n",
    "        # Store classification report metrics\n",
    "        report = classification_report(test_y, test_predictions, output_dict=True)\n",
    "        for label, values in report.items():\n",
    "            # Check if the value is a dictionary (e.g., 'precision', 'recall', 'f1-score')\n",
    "            if isinstance(values, dict):\n",
    "                for metric, value in values.items():\n",
    "                    metrics_storage[f\"{label}_{metric}\"].append(value)\n",
    "            else:\n",
    "                # Handle scalar values (like 'accuracy')\n",
    "                metrics_storage[label].append(values)\n",
    "\n",
    "        # Average the metrics over all successful runs\n",
    "        print(f\"\\nAggregated Metrics for {run+1} runs:\")\n",
    "        for metric, values in metrics_storage.items():\n",
    "            avg_value = np.mean(values)\n",
    "            print(f\"{metric}: {avg_value:.4f}\")\n",
    "\n",
    "    gc.collect()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5982a677",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/ma-bsmote/shuttle_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/ma-bsmote/shuttle_train.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/ma-datasets/shuttle_test.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_string_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[35], line 4\u001b[0m, in \u001b[0;36mrun_model\u001b[1;34m(train_data_path, test_data_path, is_string_labels, label_mapping)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_model\u001b[39m(train_data_path, test_data_path, is_string_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, label_mapping \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Load and Prepare Training Data\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Shuffle\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (is_string_labels):\n",
      "File \u001b[1;32md:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32md:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32md:\\Uni Things\\Masterarbeit\\NNs\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/ma-bsmote/shuttle_train.csv'"
     ]
    }
   ],
   "source": [
    "run_model(\"/kaggle/input/ma-datasets/shuttle_train.csv\", \"/kaggle/input/ma-datasets/shuttle_test.csv\", is_string_labels = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec3f005",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(\"/kaggle/input/ma-datasets/covtype_train.csv\", \"/kaggle/input/ma-datasets/covtype_test.csv\", is_string_labels = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57a9197",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(\"/kaggle/input/ma-datasets/kdd_train.csv\", \"/kaggle/input/ma-datasets/kdd_test.csv\", is_string_labels = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779a1e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(\"/kaggle/input/ma-datasets/darknet_train.csv\", \"/kaggle/input/ma-datasets/darknet_test.csv\", is_string_labels = False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6246813,
     "sourceId": 10123278,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1113.847549,
   "end_time": "2024-12-23T21:53:49.098246",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-23T21:35:15.250697",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
