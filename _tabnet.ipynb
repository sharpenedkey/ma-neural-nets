{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import fbeta_score, roc_auc_score, average_precision_score, matthews_corrcoef\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def import_data(file):\n",
    "    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n",
    "    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuttle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Personal\\ma-neural-nets\\.venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.51339 | val_0_logloss: 0.08558 |  0:00:03s\n",
      "epoch 1  | loss: 0.08531 | val_0_logloss: 0.04791 |  0:00:07s\n",
      "epoch 2  | loss: 0.04925 | val_0_logloss: 0.03666 |  0:00:10s\n",
      "epoch 3  | loss: 0.04239 | val_0_logloss: 0.03158 |  0:00:13s\n",
      "epoch 4  | loss: 0.03665 | val_0_logloss: 0.02854 |  0:00:17s\n",
      "epoch 5  | loss: 0.0344  | val_0_logloss: 0.02722 |  0:00:20s\n",
      "epoch 6  | loss: 0.02502 | val_0_logloss: 0.03536 |  0:00:23s\n",
      "epoch 7  | loss: 0.02482 | val_0_logloss: 0.0221  |  0:00:26s\n",
      "epoch 8  | loss: 0.02872 | val_0_logloss: 0.02489 |  0:00:29s\n",
      "epoch 9  | loss: 0.02246 | val_0_logloss: 0.02069 |  0:00:32s\n",
      "epoch 10 | loss: 0.02315 | val_0_logloss: 0.02044 |  0:00:35s\n",
      "epoch 11 | loss: 0.02013 | val_0_logloss: 0.02481 |  0:00:38s\n",
      "epoch 12 | loss: 0.02217 | val_0_logloss: 0.02095 |  0:00:41s\n",
      "epoch 13 | loss: 0.01815 | val_0_logloss: 0.01953 |  0:00:44s\n",
      "epoch 14 | loss: 0.01674 | val_0_logloss: 0.01762 |  0:00:47s\n",
      "epoch 15 | loss: 0.01833 | val_0_logloss: 0.01955 |  0:00:50s\n",
      "epoch 16 | loss: 0.01439 | val_0_logloss: 0.01719 |  0:00:53s\n",
      "epoch 17 | loss: 0.01475 | val_0_logloss: 0.01769 |  0:00:55s\n",
      "epoch 18 | loss: 0.01512 | val_0_logloss: 0.01786 |  0:00:58s\n",
      "epoch 19 | loss: 0.01377 | val_0_logloss: 0.0155  |  0:01:01s\n",
      "epoch 20 | loss: 0.01138 | val_0_logloss: 0.01595 |  0:01:05s\n",
      "epoch 21 | loss: 0.01573 | val_0_logloss: 0.01621 |  0:01:08s\n",
      "epoch 22 | loss: 0.01188 | val_0_logloss: 0.01527 |  0:01:11s\n",
      "epoch 23 | loss: 0.01373 | val_0_logloss: 0.03231 |  0:01:14s\n",
      "epoch 24 | loss: 0.01299 | val_0_logloss: 0.0156  |  0:01:17s\n",
      "epoch 25 | loss: 0.01269 | val_0_logloss: 0.01436 |  0:01:20s\n",
      "epoch 26 | loss: 0.01256 | val_0_logloss: 0.04287 |  0:01:23s\n",
      "epoch 27 | loss: 0.01648 | val_0_logloss: 0.02425 |  0:01:26s\n",
      "epoch 28 | loss: 0.01287 | val_0_logloss: 0.01238 |  0:01:29s\n",
      "epoch 29 | loss: 0.01182 | val_0_logloss: 0.01315 |  0:01:32s\n",
      "epoch 30 | loss: 0.01206 | val_0_logloss: 0.19619 |  0:01:35s\n",
      "epoch 31 | loss: 0.01822 | val_0_logloss: 0.01448 |  0:01:38s\n",
      "epoch 32 | loss: 0.01014 | val_0_logloss: 0.00901 |  0:01:41s\n",
      "epoch 33 | loss: 0.00967 | val_0_logloss: 0.0103  |  0:01:44s\n",
      "epoch 34 | loss: 0.00758 | val_0_logloss: 0.01188 |  0:01:46s\n",
      "epoch 35 | loss: 0.00846 | val_0_logloss: 0.00908 |  0:01:49s\n",
      "epoch 36 | loss: 0.00859 | val_0_logloss: 0.01172 |  0:01:52s\n",
      "epoch 37 | loss: 0.0077  | val_0_logloss: 0.01118 |  0:01:55s\n",
      "epoch 38 | loss: 0.00754 | val_0_logloss: 0.00955 |  0:01:58s\n",
      "epoch 39 | loss: 0.01041 | val_0_logloss: 0.01078 |  0:02:01s\n",
      "epoch 40 | loss: 0.01171 | val_0_logloss: 0.01914 |  0:02:04s\n",
      "epoch 41 | loss: 0.00955 | val_0_logloss: 0.01395 |  0:02:07s\n",
      "epoch 42 | loss: 0.00857 | val_0_logloss: 0.01206 |  0:02:10s\n",
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 32 and best_val_0_logloss = 0.00901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Personal\\ma-neural-nets\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 2/5\n",
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 17 and best_val_0_logloss = 0.01332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Personal\\ma-neural-nets\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 3/5\n",
      "\n",
      "Early stopping occurred at epoch 49 with best_epoch = 39 and best_val_0_logloss = 0.00999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Personal\\ma-neural-nets\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4/5\n",
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 8 and best_val_0_logloss = 0.01431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Personal\\ma-neural-nets\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 5/5\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 34 and best_val_0_logloss = 0.01114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Personal\\ma-neural-nets\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Accuracy: 0.9973\n",
      "Average F2-Score (Weighted): 0.9971\n",
      "Average AUC-ROC (One-vs-Rest): 0.9938\n",
      "Average AUC-PR: 0.9982\n",
      "Average Matthews Correlation Coefficient: 0.9924\n",
      "\n",
      "Average Classification Report:\n",
      "Class 1:\n",
      "  precision: 0.9985\n",
      "  recall: 0.9998\n",
      "  f1-score: 0.9991\n",
      "  support: 9117.0000\n",
      "Class 2:\n",
      "  precision: 0.7309\n",
      "  recall: 0.6600\n",
      "  f1-score: 0.6735\n",
      "  support: 10.0000\n",
      "Class 3:\n",
      "  precision: 0.8209\n",
      "  recall: 0.4000\n",
      "  f1-score: 0.5237\n",
      "  support: 34.0000\n",
      "Class 4:\n",
      "  precision: 0.9969\n",
      "  recall: 1.0000\n",
      "  f1-score: 0.9984\n",
      "  support: 1781.0000\n",
      "Class 5:\n",
      "  precision: 0.9918\n",
      "  recall: 0.9982\n",
      "  f1-score: 0.9950\n",
      "  support: 653.0000\n",
      "Class 6:\n",
      "  precision: 0.3333\n",
      "  recall: 0.3000\n",
      "  f1-score: 0.2933\n",
      "  support: 2.0000\n",
      "Class 7:\n",
      "  precision: 0.4000\n",
      "  recall: 0.1333\n",
      "  f1-score: 0.2000\n",
      "  support: 3.0000\n",
      "Class macro avg:\n",
      "  precision: 0.7532\n",
      "  recall: 0.6416\n",
      "  f1-score: 0.6690\n",
      "  support: 11600.0000\n",
      "Class weighted avg:\n",
      "  precision: 0.9969\n",
      "  recall: 0.9973\n",
      "  f1-score: 0.9968\n",
      "  support: 11600.0000\n"
     ]
    }
   ],
   "source": [
    "# Load the training dataset\n",
    "train_data = pd.read_csv(\"shuttle_train.csv\")\n",
    "\n",
    "# Load the test dataset\n",
    "test_data = pd.read_csv(\"shuttle_test.csv\")\n",
    "\n",
    "# Parameters\n",
    "num_runs = 5  # Number of iterations\n",
    "\n",
    "# Initialize accumulators\n",
    "accuracy_scores = []\n",
    "f2_scores = []\n",
    "auc_roc_scores = []\n",
    "auc_pr_scores = []\n",
    "mcc_scores = []\n",
    "\n",
    "# Initialize metrics accumulators for classification report\n",
    "class_metrics = defaultdict(lambda: defaultdict(float))  # Stores precision/recall/F1 for each class\n",
    "\n",
    "verbose_run = 1\n",
    "for run in range(num_runs):\n",
    "    print(f\"Run {run + 1}/{num_runs}\")\n",
    "    \n",
    "    # Shuffle data\n",
    "    train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "    test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Split features and labels\n",
    "    train_y = train_data['label'].values\n",
    "    train_X = train_data.drop(['label'], axis=1).values\n",
    "    test_y = test_data['label'].values\n",
    "    test_X = test_data.drop(['label'], axis=1).values\n",
    "    \n",
    "    # Train the model\n",
    "    clf = TabNetClassifier(verbose=verbose_run)\n",
    "    clf.fit(train_X, train_y, eval_set=[(train_X, train_y)], eval_metric=['logloss'])\n",
    "    verbose_run = 0\n",
    "    \n",
    "    # Make predictions\n",
    "    test_probabilities = clf.predict_proba(test_X)  # Outputs probabilities\n",
    "    test_predictions = clf.predict(test_X)  # Convert to discrete class predictions\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(test_y, test_predictions)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    \n",
    "    # Calculate F2-Score\n",
    "    f2_score = fbeta_score(test_y, test_predictions, beta=2, average='weighted')\n",
    "    f2_scores.append(f2_score)\n",
    "    \n",
    "    # Calculate AUC-ROC (One-vs-Rest)\n",
    "    auc_roc = roc_auc_score(test_y, test_probabilities, multi_class='ovr')\n",
    "    auc_roc_scores.append(auc_roc)\n",
    "    \n",
    "    # Calculate AUC-PR (One-vs-Rest)\n",
    "    auc_pr = average_precision_score(test_y, test_probabilities, average='weighted')\n",
    "    auc_pr_scores.append(auc_pr)\n",
    "    \n",
    "    # Calculate MCC\n",
    "    mcc = matthews_corrcoef(test_y, test_predictions)\n",
    "    mcc_scores.append(mcc)\n",
    "    \n",
    "    # Classification Report\n",
    "    report = classification_report(test_y, test_predictions, output_dict=True, zero_division=0)\n",
    "    \n",
    "    for class_label, metrics in report.items():\n",
    "        if isinstance(metrics, dict):  # Skip non-class metrics\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                class_metrics[class_label][metric_name] += metric_value\n",
    "\n",
    "# Calculate averages\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "average_f2_score = np.mean(f2_scores)\n",
    "average_auc_roc = np.mean(auc_roc_scores)\n",
    "average_auc_pr = np.mean(auc_pr_scores)\n",
    "average_mcc = np.mean(mcc_scores)\n",
    "\n",
    "# Average the classification report metrics\n",
    "average_class_metrics = {\n",
    "    class_label: {metric_name: metric_value / num_runs for metric_name, metric_value in metrics.items()}\n",
    "    for class_label, metrics in class_metrics.items()\n",
    "}\n",
    "\n",
    "# Output results\n",
    "print(f\"Average Test Accuracy: {average_accuracy:.4f}\")\n",
    "print(f\"Average F2-Score (Weighted): {average_f2_score:.4f}\")\n",
    "print(f\"Average AUC-ROC (One-vs-Rest): {average_auc_roc:.4f}\")\n",
    "print(f\"Average AUC-PR: {average_auc_pr:.4f}\")\n",
    "print(f\"Average Matthews Correlation Coefficient: {average_mcc:.4f}\")\n",
    "\n",
    "print(\"\\nAverage Classification Report:\")\n",
    "for class_label, metrics in average_class_metrics.items():\n",
    "    print(f\"Class {class_label}:\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"  {metric_name}: {metric_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covertype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Personal\\ma-neural-nets\\.venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.76758 | val_0_logloss: 0.63712 |  0:00:34s\n",
      "epoch 1  | loss: 0.62552 | val_0_logloss: 0.58473 |  0:01:08s\n",
      "epoch 2  | loss: 0.57607 | val_0_logloss: 0.53496 |  0:01:43s\n",
      "epoch 3  | loss: 0.53513 | val_0_logloss: 0.48667 |  0:02:17s\n",
      "epoch 4  | loss: 0.50821 | val_0_logloss: 0.46888 |  0:02:51s\n",
      "epoch 5  | loss: 0.49313 | val_0_logloss: 0.45239 |  0:03:25s\n",
      "epoch 6  | loss: 0.48088 | val_0_logloss: 0.44536 |  0:03:59s\n",
      "epoch 7  | loss: 0.47163 | val_0_logloss: 0.42602 |  0:04:32s\n",
      "epoch 8  | loss: 0.47321 | val_0_logloss: 0.47076 |  0:05:06s\n",
      "epoch 9  | loss: 0.45963 | val_0_logloss: 0.41327 |  0:05:40s\n",
      "epoch 10 | loss: 0.45489 | val_0_logloss: 0.41826 |  0:06:15s\n",
      "epoch 11 | loss: 0.45088 | val_0_logloss: 0.40017 |  0:06:48s\n",
      "epoch 12 | loss: 0.44245 | val_0_logloss: 0.38964 |  0:07:21s\n",
      "epoch 13 | loss: 0.43394 | val_0_logloss: 0.40391 |  0:07:55s\n",
      "epoch 14 | loss: 0.4418  | val_0_logloss: 0.42774 |  0:08:28s\n",
      "epoch 15 | loss: 0.45772 | val_0_logloss: 0.39742 |  0:09:02s\n",
      "epoch 16 | loss: 0.44395 | val_0_logloss: 0.40189 |  0:09:35s\n",
      "epoch 17 | loss: 0.44498 | val_0_logloss: 0.38895 |  0:10:09s\n",
      "epoch 18 | loss: 0.4306  | val_0_logloss: 0.37572 |  0:10:42s\n",
      "epoch 19 | loss: 0.42498 | val_0_logloss: 0.37875 |  0:11:16s\n",
      "epoch 20 | loss: 0.42108 | val_0_logloss: 0.37414 |  0:11:49s\n",
      "epoch 21 | loss: 0.41915 | val_0_logloss: 0.3666  |  0:12:23s\n",
      "epoch 22 | loss: 0.41714 | val_0_logloss: 0.36652 |  0:12:56s\n",
      "epoch 23 | loss: 0.41573 | val_0_logloss: 0.38552 |  0:13:30s\n",
      "epoch 24 | loss: 0.41527 | val_0_logloss: 0.36119 |  0:14:05s\n",
      "epoch 25 | loss: 0.41266 | val_0_logloss: 0.36944 |  0:14:39s\n",
      "epoch 26 | loss: 0.41253 | val_0_logloss: 0.36737 |  0:15:12s\n",
      "epoch 27 | loss: 0.40925 | val_0_logloss: 0.36385 |  0:15:47s\n",
      "epoch 28 | loss: 0.40805 | val_0_logloss: 0.35535 |  0:16:21s\n",
      "epoch 29 | loss: 0.41083 | val_0_logloss: 0.35346 |  0:16:58s\n",
      "epoch 30 | loss: 0.40805 | val_0_logloss: 0.35827 |  0:17:32s\n",
      "epoch 31 | loss: 0.40617 | val_0_logloss: 0.36182 |  0:18:06s\n",
      "epoch 32 | loss: 0.40514 | val_0_logloss: 0.35049 |  0:18:40s\n",
      "epoch 33 | loss: 0.40293 | val_0_logloss: 0.34997 |  0:19:13s\n",
      "epoch 34 | loss: 0.40414 | val_0_logloss: 0.35141 |  0:19:47s\n",
      "epoch 35 | loss: 0.40283 | val_0_logloss: 0.34981 |  0:20:20s\n",
      "epoch 36 | loss: 0.40241 | val_0_logloss: 0.3457  |  0:20:53s\n",
      "epoch 37 | loss: 0.40333 | val_0_logloss: 0.34678 |  0:21:27s\n",
      "epoch 38 | loss: 0.39988 | val_0_logloss: 0.343   |  0:22:01s\n",
      "epoch 39 | loss: 0.4002  | val_0_logloss: 0.3579  |  0:22:34s\n",
      "epoch 40 | loss: 0.39958 | val_0_logloss: 0.35095 |  0:23:08s\n",
      "epoch 41 | loss: 0.4017  | val_0_logloss: 0.3569  |  0:23:41s\n",
      "epoch 42 | loss: 0.40016 | val_0_logloss: 0.35379 |  0:24:15s\n",
      "epoch 43 | loss: 0.39862 | val_0_logloss: 0.34329 |  0:24:48s\n",
      "epoch 44 | loss: 0.39849 | val_0_logloss: 0.3416  |  0:25:21s\n",
      "epoch 45 | loss: 0.39673 | val_0_logloss: 0.34409 |  0:25:55s\n",
      "epoch 46 | loss: 0.3977  | val_0_logloss: 0.34785 |  0:26:28s\n",
      "epoch 47 | loss: 0.39887 | val_0_logloss: 0.33562 |  0:27:02s\n",
      "epoch 48 | loss: 0.39564 | val_0_logloss: 0.33851 |  0:27:35s\n",
      "epoch 49 | loss: 0.39549 | val_0_logloss: 0.34007 |  0:28:09s\n",
      "epoch 50 | loss: 0.39609 | val_0_logloss: 0.33579 |  0:28:42s\n",
      "epoch 51 | loss: 0.39399 | val_0_logloss: 0.34085 |  0:29:16s\n",
      "epoch 52 | loss: 0.39287 | val_0_logloss: 0.33781 |  0:29:50s\n",
      "epoch 53 | loss: 0.39198 | val_0_logloss: 0.33207 |  0:30:23s\n",
      "epoch 54 | loss: 0.39213 | val_0_logloss: 0.34232 |  0:30:57s\n",
      "epoch 55 | loss: 0.39384 | val_0_logloss: 0.33834 |  0:31:30s\n",
      "epoch 56 | loss: 0.39057 | val_0_logloss: 0.3389  |  0:32:04s\n",
      "epoch 57 | loss: 0.39107 | val_0_logloss: 0.33948 |  0:32:37s\n",
      "epoch 58 | loss: 0.39277 | val_0_logloss: 0.33424 |  0:33:10s\n",
      "epoch 59 | loss: 0.38892 | val_0_logloss: 0.3381  |  0:33:44s\n",
      "epoch 60 | loss: 0.38995 | val_0_logloss: 0.33455 |  0:34:17s\n",
      "epoch 61 | loss: 0.38911 | val_0_logloss: 0.33195 |  0:34:51s\n",
      "epoch 62 | loss: 0.38814 | val_0_logloss: 0.3349  |  0:35:24s\n",
      "epoch 63 | loss: 0.3874  | val_0_logloss: 0.32989 |  0:35:58s\n",
      "epoch 64 | loss: 0.38756 | val_0_logloss: 0.33201 |  0:36:32s\n",
      "epoch 65 | loss: 0.38729 | val_0_logloss: 0.3315  |  0:37:05s\n",
      "epoch 66 | loss: 0.38819 | val_0_logloss: 0.3263  |  0:37:38s\n",
      "epoch 67 | loss: 0.38946 | val_0_logloss: 0.3275  |  0:38:12s\n",
      "epoch 68 | loss: 0.38739 | val_0_logloss: 0.33231 |  0:38:46s\n",
      "epoch 69 | loss: 0.38809 | val_0_logloss: 0.35705 |  0:39:19s\n",
      "epoch 70 | loss: 0.38521 | val_0_logloss: 0.33065 |  0:39:53s\n",
      "epoch 71 | loss: 0.38732 | val_0_logloss: 0.32694 |  0:40:26s\n",
      "epoch 72 | loss: 0.38635 | val_0_logloss: 0.35185 |  0:41:00s\n",
      "epoch 73 | loss: 0.38546 | val_0_logloss: 0.3455  |  0:41:34s\n",
      "epoch 74 | loss: 0.38547 | val_0_logloss: 0.32897 |  0:42:07s\n",
      "epoch 75 | loss: 0.38529 | val_0_logloss: 0.32942 |  0:42:41s\n",
      "epoch 76 | loss: 0.38657 | val_0_logloss: 0.3313  |  0:43:15s\n",
      "\n",
      "Early stopping occurred at epoch 76 with best_epoch = 66 and best_val_0_logloss = 0.3263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Personal\\ma-neural-nets\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 2/5\n",
      "\n",
      "Early stopping occurred at epoch 61 with best_epoch = 51 and best_val_0_logloss = 0.3368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Personal\\ma-neural-nets\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 3/5\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 56 and best_val_0_logloss = 0.35105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Personal\\ma-neural-nets\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4/5\n",
      "\n",
      "Early stopping occurred at epoch 64 with best_epoch = 54 and best_val_0_logloss = 0.35125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Personal\\ma-neural-nets\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 5/5\n",
      "\n",
      "Early stopping occurred at epoch 62 with best_epoch = 52 and best_val_0_logloss = 0.32996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Personal\\ma-neural-nets\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Accuracy: 0.8603\n",
      "Average F2-Score (Weighted): 0.8593\n",
      "Average AUC-ROC (One-vs-Rest): 0.9824\n",
      "Average AUC-PR: 0.9324\n",
      "Average Matthews Correlation Coefficient: 0.7743\n",
      "\n",
      "Average Classification Report:\n",
      "Class 1:\n",
      "  precision: 0.8819\n",
      "  recall: 0.8328\n",
      "  f1-score: 0.8566\n",
      "  support: 42368.0000\n",
      "Class 2:\n",
      "  precision: 0.8604\n",
      "  recall: 0.9126\n",
      "  f1-score: 0.8857\n",
      "  support: 56661.0000\n",
      "Class 3:\n",
      "  precision: 0.8319\n",
      "  recall: 0.8379\n",
      "  f1-score: 0.8339\n",
      "  support: 7151.0000\n",
      "Class 4:\n",
      "  precision: 0.8310\n",
      "  recall: 0.5792\n",
      "  f1-score: 0.6797\n",
      "  support: 549.0000\n",
      "Class 5:\n",
      "  precision: 0.7646\n",
      "  recall: 0.4343\n",
      "  f1-score: 0.5522\n",
      "  support: 1899.0000\n",
      "Class 6:\n",
      "  precision: 0.6865\n",
      "  recall: 0.6813\n",
      "  f1-score: 0.6797\n",
      "  support: 3473.0000\n",
      "Class 7:\n",
      "  precision: 0.8879\n",
      "  recall: 0.8480\n",
      "  f1-score: 0.8669\n",
      "  support: 4102.0000\n",
      "Class macro avg:\n",
      "  precision: 0.8206\n",
      "  recall: 0.7323\n",
      "  f1-score: 0.7650\n",
      "  support: 116203.0000\n",
      "Class weighted avg:\n",
      "  precision: 0.8606\n",
      "  recall: 0.8603\n",
      "  f1-score: 0.8587\n",
      "  support: 116203.0000\n"
     ]
    }
   ],
   "source": [
    "# Load the training dataset\n",
    "train_data = pd.read_csv(\"covtype_train.csv\")\n",
    "\n",
    "# Load the test dataset\n",
    "test_data = pd.read_csv(\"covtype_test.csv\")\n",
    "\n",
    "# Parameters\n",
    "num_runs = 5  # Number of iterations\n",
    "\n",
    "# Initialize accumulators\n",
    "accuracy_scores = []\n",
    "f2_scores = []\n",
    "auc_roc_scores = []\n",
    "auc_pr_scores = []\n",
    "mcc_scores = []\n",
    "\n",
    "# Initialize metrics accumulators for classification report\n",
    "class_metrics = defaultdict(lambda: defaultdict(float))  # Stores precision/recall/F1 for each class\n",
    "\n",
    "verbose_run = 1\n",
    "for run in range(num_runs):\n",
    "    print(f\"Run {run + 1}/{num_runs}\")\n",
    "    \n",
    "    # Shuffle data\n",
    "    train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "    test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Split features and labels\n",
    "    train_y = train_data['label'].values\n",
    "    train_X = train_data.drop(['label'], axis=1).values\n",
    "    test_y = test_data['label'].values\n",
    "    test_X = test_data.drop(['label'], axis=1).values\n",
    "    \n",
    "    # Train the model\n",
    "    clf = TabNetClassifier(verbose=verbose_run)\n",
    "    clf.fit(train_X, train_y, eval_set=[(train_X, train_y)], eval_metric=['logloss'])\n",
    "    verbose_run = 0\n",
    "    \n",
    "    # Make predictions\n",
    "    test_probabilities = clf.predict_proba(test_X)  # Outputs probabilities\n",
    "    test_predictions = clf.predict(test_X)  # Convert to discrete class predictions\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(test_y, test_predictions)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    \n",
    "    # Calculate F2-Score\n",
    "    f2_score = fbeta_score(test_y, test_predictions, beta=2, average='weighted')\n",
    "    f2_scores.append(f2_score)\n",
    "    \n",
    "    # Calculate AUC-ROC (One-vs-Rest)\n",
    "    auc_roc = roc_auc_score(test_y, test_probabilities, multi_class='ovr')\n",
    "    auc_roc_scores.append(auc_roc)\n",
    "    \n",
    "    # Calculate AUC-PR (One-vs-Rest)\n",
    "    auc_pr = average_precision_score(test_y, test_probabilities, average='weighted')\n",
    "    auc_pr_scores.append(auc_pr)\n",
    "    \n",
    "    # Calculate MCC\n",
    "    mcc = matthews_corrcoef(test_y, test_predictions)\n",
    "    mcc_scores.append(mcc)\n",
    "    \n",
    "    # Classification Report\n",
    "    report = classification_report(test_y, test_predictions, output_dict=True, zero_division=0)\n",
    "    \n",
    "    for class_label, metrics in report.items():\n",
    "        if isinstance(metrics, dict):  # Skip non-class metrics\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                class_metrics[class_label][metric_name] += metric_value\n",
    "\n",
    "# Calculate averages\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "average_f2_score = np.mean(f2_scores)\n",
    "average_auc_roc = np.mean(auc_roc_scores)\n",
    "average_auc_pr = np.mean(auc_pr_scores)\n",
    "average_mcc = np.mean(mcc_scores)\n",
    "\n",
    "# Average the classification report metrics\n",
    "average_class_metrics = {\n",
    "    class_label: {metric_name: metric_value / num_runs for metric_name, metric_value in metrics.items()}\n",
    "    for class_label, metrics in class_metrics.items()\n",
    "}\n",
    "\n",
    "# Output results\n",
    "print(f\"Average Test Accuracy: {average_accuracy:.4f}\")\n",
    "print(f\"Average F2-Score (Weighted): {average_f2_score:.4f}\")\n",
    "print(f\"Average AUC-ROC (One-vs-Rest): {average_auc_roc:.4f}\")\n",
    "print(f\"Average AUC-PR: {average_auc_pr:.4f}\")\n",
    "print(f\"Average Matthews Correlation Coefficient: {average_mcc:.4f}\")\n",
    "\n",
    "print(\"\\nAverage Classification Report:\")\n",
    "for class_label, metrics in average_class_metrics.items():\n",
    "    print(f\"Class {class_label}:\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"  {metric_name}: {metric_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malyarchuk\\AppData\\Local\\Temp\\ipykernel_9340\\147790395.py:42: FutureWarning: The 'keep_date_col' keyword in pd.read_csv is deprecated and will be removed in a future version. Explicitly remove unwanted columns after parsing instead.\n",
      "  df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 789.51 MB\n",
      "Memory usage after optimization is: 196.60 MB\n",
      "Decreased by 75.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malyarchuk\\AppData\\Local\\Temp\\ipykernel_9340\\147790395.py:42: FutureWarning: The 'keep_date_col' keyword in pd.read_csv is deprecated and will be removed in a future version. Explicitly remove unwanted columns after parsing instead.\n",
      "  df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 197.38 MB\n",
      "Memory usage after optimization is: 49.15 MB\n",
      "Decreased by 75.1%\n",
      "Run 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Personal\\ma-neural-nets\\.venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.09035 | val_0_logloss: 0.02316 |  0:01:09s\n",
      "epoch 1  | loss: 0.01344 | val_0_logloss: 0.03259 |  0:02:20s\n",
      "epoch 2  | loss: 0.01285 | val_0_logloss: 0.02049 |  0:03:30s\n",
      "epoch 3  | loss: 0.01015 | val_0_logloss: 0.01118 |  0:04:44s\n",
      "epoch 4  | loss: 0.00892 | val_0_logloss: 0.01542 |  0:05:54s\n",
      "epoch 5  | loss: 0.00845 | val_0_logloss: 0.03439 |  0:07:04s\n",
      "epoch 6  | loss: 0.00822 | val_0_logloss: 0.03624 |  0:08:19s\n",
      "epoch 7  | loss: 0.00848 | val_0_logloss: 0.02444 |  0:09:30s\n",
      "epoch 8  | loss: 0.00887 | val_0_logloss: 0.02814 |  0:10:41s\n",
      "epoch 9  | loss: 0.00707 | val_0_logloss: 0.02183 |  0:11:51s\n",
      "epoch 10 | loss: 0.00682 | val_0_logloss: 0.04009 |  0:13:01s\n",
      "epoch 11 | loss: 0.0071  | val_0_logloss: 0.01269 |  0:14:10s\n",
      "epoch 12 | loss: 0.00626 | val_0_logloss: 0.02165 |  0:15:20s\n",
      "epoch 13 | loss: 0.00647 | val_0_logloss: 0.03427 |  0:16:30s\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_0_logloss = 0.01118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Personal\\ma-neural-nets\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 2/5\n",
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 17 and best_val_0_logloss = 0.00949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Personal\\ma-neural-nets\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 3/5\n",
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 6 and best_val_0_logloss = 0.02009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Personal\\ma-neural-nets\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4/5\n",
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_0_logloss = 0.01522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Personal\\ma-neural-nets\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 5/5\n",
      "\n",
      "Early stopping occurred at epoch 38 with best_epoch = 28 and best_val_0_logloss = 0.00646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Personal\\ma-neural-nets\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Accuracy: 0.9971\n",
      "Average F2-Score (Weighted): 0.9969\n",
      "Average AUC-ROC (One-vs-Rest): 0.9837\n",
      "Average AUC-PR: 0.9987\n",
      "Average Matthews Correlation Coefficient: 0.9665\n",
      "\n",
      "Average Classification Report:\n",
      "Class back.:\n",
      "  precision: 0.9681\n",
      "  recall: 0.4916\n",
      "  f1-score: 0.5951\n",
      "  support: 441.0000\n",
      "Class buffer_overflow.:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 6.0000\n",
      "Class ftp_write.:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 2.0000\n",
      "Class guess_passwd.:\n",
      "  precision: 0.6119\n",
      "  recall: 0.7273\n",
      "  f1-score: 0.6623\n",
      "  support: 11.0000\n",
      "Class imap.:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 2.0000\n",
      "Class ipsweep.:\n",
      "  precision: 0.9844\n",
      "  recall: 0.9812\n",
      "  f1-score: 0.9828\n",
      "  support: 2496.0000\n",
      "Class land.:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 4.0000\n",
      "Class loadmodule.:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 2.0000\n",
      "Class multihop.:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 1.0000\n",
      "Class nmap.:\n",
      "  precision: 0.9462\n",
      "  recall: 0.9637\n",
      "  f1-score: 0.9548\n",
      "  support: 463.0000\n",
      "Class normal.:\n",
      "  precision: 0.9978\n",
      "  recall: 0.9996\n",
      "  f1-score: 0.9987\n",
      "  support: 194557.0000\n",
      "Class perl.:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 1.0000\n",
      "Class phf.:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 1.0000\n",
      "Class pod.:\n",
      "  precision: 0.4798\n",
      "  recall: 0.5170\n",
      "  f1-score: 0.4943\n",
      "  support: 53.0000\n",
      "Class portsweep.:\n",
      "  precision: 0.9928\n",
      "  recall: 0.9829\n",
      "  f1-score: 0.9878\n",
      "  support: 2083.0000\n",
      "Class rootkit.:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 2.0000\n",
      "Class satan.:\n",
      "  precision: 0.9939\n",
      "  recall: 0.9858\n",
      "  f1-score: 0.9898\n",
      "  support: 3178.0000\n",
      "Class spy.:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 1.0000\n",
      "Class teardrop.:\n",
      "  precision: 0.8914\n",
      "  recall: 0.9898\n",
      "  f1-score: 0.9352\n",
      "  support: 196.0000\n",
      "Class warezclient.:\n",
      "  precision: 0.8254\n",
      "  recall: 0.5529\n",
      "  f1-score: 0.6490\n",
      "  support: 204.0000\n",
      "Class warezmaster.:\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1-score: 0.0000\n",
      "  support: 4.0000\n",
      "Class macro avg:\n",
      "  precision: 0.4139\n",
      "  recall: 0.3901\n",
      "  f1-score: 0.3929\n",
      "  support: 203708.0000\n",
      "Class weighted avg:\n",
      "  precision: 0.9968\n",
      "  recall: 0.9971\n",
      "  f1-score: 0.9966\n",
      "  support: 203708.0000\n"
     ]
    }
   ],
   "source": [
    "# Load the training dataset\n",
    "train_data = import_data(\"kdd_train.csv\")\n",
    "\n",
    "# Load the test dataset\n",
    "test_data = import_data(\"kdd_test.csv\")\n",
    "\n",
    "# Parameters\n",
    "num_runs = 5  # Number of iterations\n",
    "\n",
    "# Initialize accumulators\n",
    "accuracy_scores = []\n",
    "f2_scores = []\n",
    "auc_roc_scores = []\n",
    "auc_pr_scores = []\n",
    "mcc_scores = []\n",
    "\n",
    "# Initialize metrics accumulators for classification report\n",
    "class_metrics = defaultdict(lambda: defaultdict(float))  # Stores precision/recall/F1 for each class\n",
    "\n",
    "verbose_run = 1\n",
    "for run in range(num_runs):\n",
    "    print(f\"Run {run + 1}/{num_runs}\")\n",
    "    \n",
    "    # Shuffle data\n",
    "    train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "    test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Split features and labels\n",
    "    train_y = train_data['label'].values\n",
    "    train_X = train_data.drop(['label'], axis=1).values\n",
    "    test_y = test_data['label'].values\n",
    "    test_X = test_data.drop(['label'], axis=1).values\n",
    "    \n",
    "    # Train the model\n",
    "    clf = TabNetClassifier(verbose=verbose_run)\n",
    "    clf.fit(train_X, train_y, eval_set=[(train_X, train_y)], eval_metric=['logloss'])\n",
    "    verbose_run = 0\n",
    "    \n",
    "    # Make predictions\n",
    "    test_probabilities = clf.predict_proba(test_X)  # Outputs probabilities\n",
    "    test_predictions = clf.predict(test_X)  # Convert to discrete class predictions\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(test_y, test_predictions)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    \n",
    "    # Calculate F2-Score\n",
    "    f2_score = fbeta_score(test_y, test_predictions, beta=2, average='weighted')\n",
    "    f2_scores.append(f2_score)\n",
    "    \n",
    "    # Calculate AUC-ROC (One-vs-Rest)\n",
    "    auc_roc = roc_auc_score(test_y, test_probabilities, multi_class='ovr')\n",
    "    auc_roc_scores.append(auc_roc)\n",
    "    \n",
    "    # Calculate AUC-PR (One-vs-Rest)\n",
    "    auc_pr = average_precision_score(test_y, test_probabilities, average='weighted')\n",
    "    auc_pr_scores.append(auc_pr)\n",
    "    \n",
    "    # Calculate MCC\n",
    "    mcc = matthews_corrcoef(test_y, test_predictions)\n",
    "    mcc_scores.append(mcc)\n",
    "    \n",
    "    # Classification Report\n",
    "    report = classification_report(test_y, test_predictions, output_dict=True, zero_division=0)\n",
    "    \n",
    "    for class_label, metrics in report.items():\n",
    "        if isinstance(metrics, dict):  # Skip non-class metrics\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                class_metrics[class_label][metric_name] += metric_value\n",
    "\n",
    "# Calculate averages\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "average_f2_score = np.mean(f2_scores)\n",
    "average_auc_roc = np.mean(auc_roc_scores)\n",
    "average_auc_pr = np.mean(auc_pr_scores)\n",
    "average_mcc = np.mean(mcc_scores)\n",
    "\n",
    "# Average the classification report metrics\n",
    "average_class_metrics = {\n",
    "    class_label: {metric_name: metric_value / num_runs for metric_name, metric_value in metrics.items()}\n",
    "    for class_label, metrics in class_metrics.items()\n",
    "}\n",
    "\n",
    "# Output results\n",
    "print(f\"Average Test Accuracy: {average_accuracy:.4f}\")\n",
    "print(f\"Average F2-Score (Weighted): {average_f2_score:.4f}\")\n",
    "print(f\"Average AUC-ROC (One-vs-Rest): {average_auc_roc:.4f}\")\n",
    "print(f\"Average AUC-PR: {average_auc_pr:.4f}\")\n",
    "print(f\"Average Matthews Correlation Coefficient: {average_mcc:.4f}\")\n",
    "\n",
    "print(\"\\nAverage Classification Report:\")\n",
    "for class_label, metrics in average_class_metrics.items():\n",
    "    print(f\"Class {class_label}:\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"  {metric_name}: {metric_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Darknet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Personal\\ma-neural-nets\\.venv\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.35467 | val_0_logloss: 0.25503 |  0:00:09s\n",
      "epoch 1  | loss: 0.11494 | val_0_logloss: 0.09692 |  0:00:19s\n",
      "epoch 2  | loss: 0.08185 | val_0_logloss: 0.06624 |  0:00:29s\n",
      "epoch 3  | loss: 0.06231 | val_0_logloss: 0.05281 |  0:00:39s\n",
      "epoch 4  | loss: 0.04829 | val_0_logloss: 0.03999 |  0:00:49s\n",
      "epoch 5  | loss: 0.04389 | val_0_logloss: 0.03575 |  0:00:58s\n",
      "epoch 6  | loss: 0.05056 | val_0_logloss: 0.05347 |  0:01:08s\n",
      "epoch 7  | loss: 0.04927 | val_0_logloss: 0.03098 |  0:01:18s\n",
      "epoch 8  | loss: 0.03203 | val_0_logloss: 0.04191 |  0:01:28s\n",
      "epoch 9  | loss: 0.0295  | val_0_logloss: 0.02437 |  0:01:38s\n",
      "epoch 10 | loss: 0.02792 | val_0_logloss: 0.02296 |  0:01:48s\n",
      "epoch 11 | loss: 0.02542 | val_0_logloss: 0.02088 |  0:01:58s\n",
      "epoch 12 | loss: 0.0238  | val_0_logloss: 0.01965 |  0:02:08s\n",
      "epoch 13 | loss: 0.02198 | val_0_logloss: 0.01808 |  0:02:18s\n",
      "epoch 14 | loss: 0.01941 | val_0_logloss: 0.01739 |  0:02:27s\n",
      "epoch 15 | loss: 0.02    | val_0_logloss: 0.01809 |  0:02:37s\n",
      "epoch 16 | loss: 0.01901 | val_0_logloss: 0.01495 |  0:02:47s\n",
      "epoch 17 | loss: 0.01753 | val_0_logloss: 0.01812 |  0:02:57s\n",
      "epoch 18 | loss: 0.01723 | val_0_logloss: 0.01784 |  0:03:07s\n",
      "epoch 19 | loss: 0.01612 | val_0_logloss: 0.01709 |  0:03:17s\n",
      "epoch 20 | loss: 0.0142  | val_0_logloss: 0.01586 |  0:03:27s\n",
      "epoch 21 | loss: 0.01427 | val_0_logloss: 0.0191  |  0:03:37s\n",
      "epoch 22 | loss: 0.02445 | val_0_logloss: 0.02049 |  0:03:47s\n",
      "epoch 23 | loss: 0.01716 | val_0_logloss: 0.01114 |  0:03:57s\n",
      "epoch 24 | loss: 0.01395 | val_0_logloss: 0.01292 |  0:04:07s\n",
      "epoch 25 | loss: 0.01395 | val_0_logloss: 0.012   |  0:04:16s\n",
      "epoch 26 | loss: 0.01353 | val_0_logloss: 0.0346  |  0:04:26s\n",
      "epoch 27 | loss: 0.01645 | val_0_logloss: 0.0135  |  0:04:36s\n",
      "epoch 28 | loss: 0.01358 | val_0_logloss: 0.04903 |  0:04:46s\n",
      "epoch 29 | loss: 0.01091 | val_0_logloss: 0.01389 |  0:04:56s\n",
      "epoch 30 | loss: 0.01051 | val_0_logloss: 0.02413 |  0:05:06s\n",
      "epoch 31 | loss: 0.00969 | val_0_logloss: 0.00748 |  0:05:16s\n",
      "epoch 32 | loss: 0.00901 | val_0_logloss: 0.03693 |  0:05:26s\n",
      "epoch 33 | loss: 0.01107 | val_0_logloss: 0.00797 |  0:05:36s\n",
      "epoch 34 | loss: 0.0094  | val_0_logloss: 0.06413 |  0:05:46s\n",
      "epoch 35 | loss: 0.01807 | val_0_logloss: 0.09221 |  0:05:56s\n",
      "epoch 36 | loss: 0.01252 | val_0_logloss: 0.00716 |  0:06:06s\n",
      "epoch 37 | loss: 0.01171 | val_0_logloss: 0.01397 |  0:06:16s\n",
      "epoch 38 | loss: 0.01242 | val_0_logloss: 0.05865 |  0:06:26s\n",
      "epoch 39 | loss: 0.01196 | val_0_logloss: 0.04855 |  0:06:36s\n",
      "epoch 40 | loss: 0.02028 | val_0_logloss: 0.04842 |  0:06:46s\n",
      "epoch 41 | loss: 0.00907 | val_0_logloss: 0.00676 |  0:06:56s\n",
      "epoch 42 | loss: 0.00856 | val_0_logloss: 0.00744 |  0:07:06s\n",
      "epoch 43 | loss: 0.00906 | val_0_logloss: 0.00668 |  0:07:16s\n",
      "epoch 44 | loss: 0.00803 | val_0_logloss: 0.06897 |  0:07:26s\n",
      "epoch 45 | loss: 0.00911 | val_0_logloss: 0.00566 |  0:07:36s\n",
      "epoch 46 | loss: 0.007   | val_0_logloss: 0.00676 |  0:07:46s\n",
      "epoch 47 | loss: 0.00889 | val_0_logloss: 0.03984 |  0:07:55s\n",
      "epoch 48 | loss: 0.00968 | val_0_logloss: 0.00531 |  0:08:06s\n",
      "epoch 49 | loss: 0.00686 | val_0_logloss: 0.04512 |  0:08:16s\n",
      "epoch 50 | loss: 0.00706 | val_0_logloss: 0.01308 |  0:08:29s\n",
      "epoch 51 | loss: 0.00687 | val_0_logloss: 0.02588 |  0:08:41s\n",
      "epoch 52 | loss: 0.00609 | val_0_logloss: 0.02746 |  0:08:52s\n",
      "epoch 53 | loss: 0.00597 | val_0_logloss: 0.01778 |  0:09:02s\n",
      "epoch 54 | loss: 0.00602 | val_0_logloss: 0.00533 |  0:09:12s\n",
      "epoch 55 | loss: 0.00799 | val_0_logloss: 0.01921 |  0:09:22s\n",
      "epoch 56 | loss: 0.00903 | val_0_logloss: 0.05074 |  0:09:32s\n",
      "epoch 57 | loss: 0.00919 | val_0_logloss: 0.02328 |  0:09:42s\n",
      "epoch 58 | loss: 0.00569 | val_0_logloss: 0.06447 |  0:09:52s\n",
      "\n",
      "Early stopping occurred at epoch 58 with best_epoch = 48 and best_val_0_logloss = 0.00531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Personal\\ma-neural-nets\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 2/5\n",
      "\n",
      "Early stopping occurred at epoch 58 with best_epoch = 48 and best_val_0_logloss = 0.0035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Personal\\ma-neural-nets\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 3/5\n",
      "\n",
      "Early stopping occurred at epoch 53 with best_epoch = 43 and best_val_0_logloss = 0.00921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Personal\\ma-neural-nets\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4/5\n",
      "\n",
      "Early stopping occurred at epoch 67 with best_epoch = 57 and best_val_0_logloss = 0.00434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Personal\\ma-neural-nets\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 5/5\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_0_logloss = 0.03094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Personal\\ma-neural-nets\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Accuracy: 0.9969\n",
      "Average F2-Score (Weighted): 0.9969\n",
      "Average AUC-ROC (One-vs-Rest): 0.9995\n",
      "Average AUC-PR: 0.9989\n",
      "Average Matthews Correlation Coefficient: 0.9886\n",
      "\n",
      "Average Classification Report:\n",
      "Class Darknet_Audio-Streaming:\n",
      "  precision: 0.9960\n",
      "  recall: 0.9856\n",
      "  f1-score: 0.9908\n",
      "  support: 2657.0000\n",
      "Class Darknet_Browsing:\n",
      "  precision: 0.7807\n",
      "  recall: 0.7849\n",
      "  f1-score: 0.7813\n",
      "  support: 53.0000\n",
      "Class Darknet_Chat:\n",
      "  precision: 0.9788\n",
      "  recall: 0.9866\n",
      "  f1-score: 0.9826\n",
      "  support: 908.0000\n",
      "Class Darknet_Email:\n",
      "  precision: 0.9948\n",
      "  recall: 0.9914\n",
      "  f1-score: 0.9931\n",
      "  support: 116.0000\n",
      "Class Darknet_File-Transfer:\n",
      "  precision: 0.9829\n",
      "  recall: 0.9885\n",
      "  f1-score: 0.9856\n",
      "  support: 522.0000\n",
      "Class Darknet_P2P:\n",
      "  precision: 0.9357\n",
      "  recall: 0.9227\n",
      "  f1-score: 0.9254\n",
      "  support: 44.0000\n",
      "Class Darknet_VOIP:\n",
      "  precision: 0.9867\n",
      "  recall: 0.9631\n",
      "  f1-score: 0.9747\n",
      "  support: 293.0000\n",
      "Class Darknet_Video-Streaming:\n",
      "  precision: 0.9408\n",
      "  recall: 0.9502\n",
      "  f1-score: 0.9450\n",
      "  support: 269.0000\n",
      "Class Normal:\n",
      "  precision: 0.9991\n",
      "  recall: 0.9999\n",
      "  f1-score: 0.9995\n",
      "  support: 26862.0000\n",
      "Class macro avg:\n",
      "  precision: 0.9551\n",
      "  recall: 0.9525\n",
      "  f1-score: 0.9531\n",
      "  support: 31724.0000\n",
      "Class weighted avg:\n",
      "  precision: 0.9969\n",
      "  recall: 0.9969\n",
      "  f1-score: 0.9969\n",
      "  support: 31724.0000\n"
     ]
    }
   ],
   "source": [
    "# Load the training dataset\n",
    "train_data = pd.read_csv(\"darknet_train.csv\")\n",
    "\n",
    "# Load the test dataset\n",
    "test_data = pd.read_csv(\"darknet_test.csv\")\n",
    "\n",
    "# Parameters\n",
    "num_runs = 5  # Number of iterations\n",
    "\n",
    "# Initialize accumulators\n",
    "accuracy_scores = []\n",
    "f2_scores = []\n",
    "auc_roc_scores = []\n",
    "auc_pr_scores = []\n",
    "mcc_scores = []\n",
    "\n",
    "# Initialize metrics accumulators for classification report\n",
    "class_metrics = defaultdict(lambda: defaultdict(float))  # Stores precision/recall/F1 for each class\n",
    "\n",
    "verbose_run = 1\n",
    "for run in range(num_runs):\n",
    "    print(f\"Run {run + 1}/{num_runs}\")\n",
    "    \n",
    "    # Shuffle data\n",
    "    train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "    test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Split features and labels\n",
    "    train_y = train_data['label'].values\n",
    "    train_X = train_data.drop(['label'], axis=1).values\n",
    "    test_y = test_data['label'].values\n",
    "    test_X = test_data.drop(['label'], axis=1).values\n",
    "    \n",
    "    # Train the model\n",
    "    clf = TabNetClassifier(verbose=verbose_run)\n",
    "    clf.fit(train_X, train_y, eval_set=[(train_X, train_y)], eval_metric=['logloss'])\n",
    "    verbose_run = 0\n",
    "    \n",
    "    # Make predictions\n",
    "    test_probabilities = clf.predict_proba(test_X)  # Outputs probabilities\n",
    "    test_predictions = clf.predict(test_X)  # Convert to discrete class predictions\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(test_y, test_predictions)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    \n",
    "    # Calculate F2-Score\n",
    "    f2_score = fbeta_score(test_y, test_predictions, beta=2, average='weighted')\n",
    "    f2_scores.append(f2_score)\n",
    "    \n",
    "    # Calculate AUC-ROC (One-vs-Rest)\n",
    "    auc_roc = roc_auc_score(test_y, test_probabilities, multi_class='ovr')\n",
    "    auc_roc_scores.append(auc_roc)\n",
    "    \n",
    "    # Calculate AUC-PR (One-vs-Rest)\n",
    "    auc_pr = average_precision_score(test_y, test_probabilities, average='weighted')\n",
    "    auc_pr_scores.append(auc_pr)\n",
    "    \n",
    "    # Calculate MCC\n",
    "    mcc = matthews_corrcoef(test_y, test_predictions)\n",
    "    mcc_scores.append(mcc)\n",
    "    \n",
    "    # Classification Report\n",
    "    report = classification_report(test_y, test_predictions, output_dict=True, zero_division=0)\n",
    "    \n",
    "    for class_label, metrics in report.items():\n",
    "        if isinstance(metrics, dict):  # Skip non-class metrics\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                class_metrics[class_label][metric_name] += metric_value\n",
    "\n",
    "# Calculate averages\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "average_f2_score = np.mean(f2_scores)\n",
    "average_auc_roc = np.mean(auc_roc_scores)\n",
    "average_auc_pr = np.mean(auc_pr_scores)\n",
    "average_mcc = np.mean(mcc_scores)\n",
    "\n",
    "# Average the classification report metrics\n",
    "average_class_metrics = {\n",
    "    class_label: {metric_name: metric_value / num_runs for metric_name, metric_value in metrics.items()}\n",
    "    for class_label, metrics in class_metrics.items()\n",
    "}\n",
    "\n",
    "# Output results\n",
    "print(f\"Average Test Accuracy: {average_accuracy:.4f}\")\n",
    "print(f\"Average F2-Score (Weighted): {average_f2_score:.4f}\")\n",
    "print(f\"Average AUC-ROC (One-vs-Rest): {average_auc_roc:.4f}\")\n",
    "print(f\"Average AUC-PR: {average_auc_pr:.4f}\")\n",
    "print(f\"Average Matthews Correlation Coefficient: {average_mcc:.4f}\")\n",
    "\n",
    "print(\"\\nAverage Classification Report:\")\n",
    "for class_label, metrics in average_class_metrics.items():\n",
    "    print(f\"Class {class_label}:\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"  {metric_name}: {metric_value:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
