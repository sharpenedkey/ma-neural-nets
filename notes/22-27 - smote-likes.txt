22-smote-no-val:
* SMOTE
* no validation
* batch size of 0,5% of dataset
* categorical cross-entropy loss

23-smote-5-fold-val:
* SMOTE
* 5-fold validation
* batch size of 0,5% of dataset
* categorical cross-entropy loss

24-bord-smote-no-val:
* BorderlineSMOTE
* no validation
* batch size of 0,5% of dataset
* categorical cross-entropy loss

25-bord-smote-5-fold-val:
* BorderlineSMOTE
* 5-fold validation
* batch size of 0,5% of dataset
* categorical cross-entropy loss

26-bsmote-fl-no-val:
* BorderlineSMOTE
* no validation
* batch size of 0,5% of dataset
* focal loss

27-bsmote-fl-5-fold-val:
* BorderlineSMOTE
* 5-fold validation
* batch size of 0,5% of dataset
* focal loss


=>

First we tried ADASYN. There was need to define an upper limit to each class,
since ADASYN estimates the expected size itself based on the distribution of feature values.
ADASYN returned an error "Not any neigbours belong to the majority class.
This case will induce a NaN case with a division by zero.
ADASYN is not suited for this specific dataset. Use SMOTE instead."
This was the case for the KDD set. Since ADASYN isn't applicable to one of the datasets,
we take it out of consideration.

SMOTE-NC wasn't applicable: we've already converted all features into numerical.
The only thing SMOTE-NC does is one-hot encode the categorical features, which we already did.

KMeansSMOTE was unable to find with sufficient samples for many classes of the Shuttle dataset.
Therefore we lowered the cluster_balance_threshold to 0, such that it could start in the first place.
Still, the KDD dataset failed with an error "No clusters found with sufficient samples of class perl.
Try lowering the cluster_balance_threshold or increasing the number of clusters."
Since the threshold was already minimal and we couldn't increase the amount of clusters any higher
(as there were only 2 entries for some classes in the KDD dataset), we discraded KMeansSMOTE as 
a resampling algorithm.

SVMSMOTE turned out to be prohibitively slow. So much so that doing the resampling before every benchmark
would take days. Instead, we first resampled the data in separate Jupyter notebooks, and then
loaded it into our benchmarking notebooks.
First we were able to resample the Shuttle and Covertype datasets. When benchmarking those, almost all metrics
turned out to be worse than for the architecture iterations where no rebalancing mechanism was used, or even
where SMOTE was utilized. The only exceptions were the F2, the Weighted Average Recall and F1 Score for the
Darknet dataset.
The duration of resampling for the Covertype dataset was above 12 hours. In a business context,
performance is one of the primal
concerns of any business case: rebalancing approaches that take ten times as much space and require
days to run (slower than the other approaches by many factors) are not that. Though rebalancing the Shuttle
and Darknet datasets took less time, these, still, underperformed on almost all metrics, especially the macro
metrics -- even though improving these metrics was one of the main reasons for us attempting to use
rebalancing.
Finally, as for the KDD dataset, the algorithm could not find any support vectors and considered all
features to be noise-like, which lead to an abrupt stop to the execution:
ValueError: All support vectors are considered as noise. SVM-SMOTE is not adapted to your dataset. Try another SMOTE variant.

In conclusion, it was decided that we would not use any rebalancing mechanisms that modify the datasets.
Not only did it not improve the performance of the architecture, but it also delegated the purpose of building
a neural network capable of processing imbalanced datasets onto a separate pre-processing element.