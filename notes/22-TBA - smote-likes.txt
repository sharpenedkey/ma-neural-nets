With:
* SMOTE
* BorderlineSMOTE
* SVMSMOTE

Then maybe try looking at whether cross-loss or focal loss is better.

22-smote-no-val:
* SMOTE
* no validation
* batch size of 0,5% of dataset
* categorical cross-entropy loss

23-smote-5-fold-val:
* SMOTE
* 5-fold validation
* batch size of 0,5% of dataset
* categorical cross-entropy loss

24-bord-smote-no-val:
* BorderlineSMOTE
* no validation
* batch size of 0,5% of dataset
* categorical cross-entropy loss

25-bord-smote-5-fold-val:
* BorderlineSMOTE
* 5-fold validation
* batch size of 0,5% of dataset
* categorical cross-entropy loss


=>

First we tried ADASYN. There was need to define an upper limit to each class,
since ADASYN estimates the expected size itself based on the distribution of feature values.
ADASYN returned an error "Not any neigbours belong to the majority class.
This case will induce a NaN case with a division by zero.
ADASYN is not suited for this specific dataset. Use SMOTE instead."
This was the case for the KDD set. Since ADASYN isn't applicable to one of the datasets,
we take it out of consideration.

SMOTE-NC wasn't applicable: we've already converted all features into numerical.
The only thing SMOTE-NC does is one-hot encode the categorical features, which we already did.

KMeansSMOTE was unable to find with sufficient samples for many classes of the Shuttle dataset.
Therefore we lowered the cluster_balance_threshold to 0, such that it could start in the first place.
Still, the KDD dataset failed with an error "No clusters found with sufficient samples of class perl.
Try lowering the cluster_balance_threshold or increasing the number of clusters."
Since the threshold was already minimal and we couldn't increase the amount of clusters any higher
(as there were only 2 entries for some classes in the KDD dataset), we discraded KMeansSMOTE as 
a resampling algorithm.

SVMSMOTE turned out to be prohibitively slow. So much so that doing the resampling before every benchmark
would take days. Instead, we first resampled the data in separate Jupyter notebooks, and then
loaded it into our benchmarking notebooks.
First we were able to resample the Shuttle and Covertype datasets. When bencdhmarking those, the metrics
turned out to be worse than for the architecture iterations where no rebalancing mechanism was used.
The duration of resampling for the KDD and Covertype datasets was above 12 hours, which led us to conclude
that continuing this experiment wasn't worth it. In a business context, performance is one of the primal
concerns of any business case: rebalancing approaches that take ten times as much space and require
days to run (slower than the other approaches by many factors) are not that.