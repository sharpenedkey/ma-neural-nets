28-1d-conv-dynamic:
* amount of layers depends on amount of features
* at least 2 layers, at most floor[features/10]
* e.g. for shuttle, 2 layers (only 9 features)
  covertype: 54 features => 5 layers
  kdd: 126 features => 12 layers
  darknet: 103 features => 10 layers
  shuttle, 2 layers (only 9 features)
* first layer has features*2 filters
* each subsequent layer has twice as less filters as the previous
* each layer has at least 16 filters (sliding windows)

29-1d-conv-dyn-less-layers:
* like 28, but with less layers total
* minimum is 1 layer
* maximum is floor[features/20]
* e.g. for shuttle, 1 layer
  covertype: 54 features => 2 layers
  kdd: 126 features => 6 layers
  darknet: 103 features => 5 layers

29-1d-4-layers-decr-filters:
* always four layers
* filter number decreases over time:
  layer 1: int(max(16, input_dim * 2)) layers
  layer 2: input_dim
  layer 3: int(max(16, input_dim // 2))
  layer 4: int(max(16, input_dim // 4))


=>

First we attempted to adapt ConvNeXt to our use case.
ConvNeXt, however, much like any other visual neural network,
is built for recognizing images. As such, every already existing
framework that implements this model also expects image data,
randing anywhere from 32x32 px to 324x324 px, with 3 color
channels in each image (essentially expecting 3D data).
Our datasets, however, are all one-dimensional. Adapting them
to ConvNeXt's expectations would mean modifying the data structure
itself, either by padding the missing values with zeroes or by
first extracting some kinds of numerical features that could then
be plotted onto this three-dimensional grid. The quality of this approach
would largely depend on the nature of the dataset in question. Datasets
like Shuttle, that have only 9 feature columns in total, would probably
have to be padded in order to avoid introducing unnecessary artifacts.
Others, like KDD (with 126 features), were under a much lower risk of
artifact introduction, but one that was still non-zero.

As such, we instead decided to improve on our original NN architecture
by taking heavy inspiration from ConvNeXt.

Iteration 28 has a bit worse performance for KDD, among others. That can 
theoretically be explained by the urge to over-generalize in the 28th architecture 
version, since KDD would bring about 12 layers in that architecture and only
6 in the 29th version. However, some benchmarks are very close in both architecture
iterations, meaning that this over-generalization doesn't happen at all times.
Regardless, more layers doesn't necessarily mean a better architecture, as can
be seen in this case. 12 layers have never brought better performance than only
6, but have indeed lead to higher runtimes.

The 29th version was better than the 28th in almost all situations, save for
Covertype. The 28th version lead to it being trained on a NN with 5 layers,
while the 29th, due to the nature of computing the amount of necessary layers,
had only 2 convolutional layers.

This lead to the idea that, perhaps, making the amount of layers dependent on the
width of the used dataset was a wrong idea in the first place.

The 30th iteration aimed to circumvent this issue by setting the amount of layers
to a constant of four. The decision was also made to decrease the amount of filters
(and as such, extracted features) in each subsequent layer. This had multiple reasons,
the largest one being runtime constraints.

If the amount of features were to increase
in each subsequent layer, this would have very negatively impacted the runtime of
the training process and benchmarks as a whole. In other architecture versions that
didn't make the final cut, the amount of filters was instead either doubled in each
subsequent layer, or increased by 50%. This more than doubled the runtime necessary
to train at least one neural network on the KDD dataset, resulting in requiring more
than 48 hours to go through just 80 training epochs on a modern PC (CPU i9-9980HK
with 64 GB of RAM) -- and this for just one training run. This would have made benchmarking
the architecture and iteratively improving on it nigh impossible. Even though the
accuracy and the loss function were promising during training, this is an approach that
was ultimately discarded as too demanding on the hardware. After all, the neural networks
being developed were only supposed to be one part of the overall architecture, acting
more as feature extractors than anything else. Thus, the decision was to instead opt
for an architecture with a decreasing amount of filters. These extracted features
could then be combined with the features of the original datasets to be then fed to
a classifying/boosting algorithm like CatBoost or XGBoost.